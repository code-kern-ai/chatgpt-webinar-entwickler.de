{"content": ["Refinery allows you to calculate new attributes to your already existing data. That means that you can write Python code that takes your existing record as input, does some transformations or uses outside APIs, and finally returns a value that is the value for this new attribute. The attribute calculation is applied to every record individually.", "We strongly believe that a good labeling environment has to be as flexible as possible in order to keep up with changing requirements. One of those tools enabling more flexibility is calculating new attributes while the project is already up and running. \n\nThis is useful for many scenarios, here are two examples:\n\n- You engineer the same features in many of your labeling functions so you decide to take the code and add it as a new attribute, which saves you a lot of computation in the weak supervision cycle.\n- While labeling you find that a useful predictor could be the sentiment of a given text. You decide to add a new attribute to your data that enriches your records with a sentiment score from a remote API that you know and trust.\n\nFor more inspiration, check out our content library called [bricks](https://bricks.kern.ai/home), where we collected commonly used NLP enrichments like profanity detection, E-Mail extraction, language translation, and many many more. They are all written in Python and designed to be copied into refinery directly.", "To add a new attribute to your data, you have to visit the settings page and click the button \"add new attribute\" (see Fig.1).\n<CaptionedImage src=\"adding-attributes/adding_attributes_1.png\" title=\"Fig. 1: Screenshot of the settings page where the user is about to add a new attribute.\"/>\nAfter that, a modal will appear (see Fig. 2), which prompts you to input a new unique attribute name and the data type of that new attribute.\n\n<CaptionedImage src=\"adding-attributes/adding_attributes_2.png\" title=\"Fig. 2: Screenshot of the modal that appears after clicking 'add new attribute'.\"/>\n\n<Note>\nOnly `text` attributes are available for tokenization and embedding creation.\n</Note>\n\nThe attribute data type acts as a safeguard for your calculated attribute because the calculation will throw an error when the returned value does not match the specified data type. Also, the data type will be considered in the data browser, so every data type has different filtering options. For example, an attribute of the boolean data type can only be toggled on or off during filtering, there are no additional options for it. If you want to know more about the effects of selecting the data types, please read the section about [attribute filters](/refinery/data-management#attribute-filters).\n\n<CaptionedImage src=\"adding-attributes/adding_attributes_3.png\" title=\"Fig. 3: Screenshot of an attribute calculation function that adds a new attribute, which is a list of people that are referenced in the attribute 'headline'. Notice that this function has not been run yet and therefore displays the state 'Registered' next to its name at the top.\"/>\nAfter creating the function, we are greeted by the usual interface that you should be familiar with from the [labeling functions](/refinery/building-labeling-functions). The input that you get is still a single record, that can be treated as a dictionary, where every text attribute is of type `spaCy Doc` (see [their documentation](https://spacy.io/api/doc)). That is why we can iterate through the entities of the attribute \"headline\" that were extracted during the tokenization process.\n\n<Note>\nYou can use requests in your attribute calculation.\nWhen you want to enrich your records with an outside or local running API, then you can use the [requests Python package](https://pypi.org/project/requests/) in your attribute calculation function.\n</Note>\nAfter assembling your attribute in the function you just have to return it. Make sure that the data type matches the one you specified because otherwise you will get an error. Good practice is to use the \"run on 10\" feature before committing to the attribute calculation, as this will usually clear up any bugs.\n\nWhen everything is finalized and you tested your function, you can press on \"run\" in the bottom right corner. This will execute the attribute calculation function on your whole data. Please note that this calculated attribute is immutable after execution, so if you need to adjust something, you will need to create a new attribute and copy the old code.\n<CaptionedImage src=\"adding-attributes/adding_attributes_4.png\" title=\"Fig. 4: After execution, the new attribute is now immutable and in the state 'usable'.\"/>\n\n<CaptionedImage src=\"adding-attributes/adding_attributes_5.png\" title=\"Fig. 5: A screenshot of a record in the data browser, where we can see the newly added attribute.\"/>", "Attribute visibility refers to the concept of hiding specific attributes that are not useful for the user to constantly see.\n<CaptionedImage src=\"/refinery/attribute-visibility/1.png\" title=\"Fig. 1: Screenshot of the settings page where the user can select the attribute visibility in a dropdown.\"/>", "`Do not hide`: This is the default option and has no effect on any display in refinery. The attribute is visible everywhere. Use this for attributes that are adding valuable information to your records that you want to see at all times.\n\n`Hide on data browser`: This option will hide the attribute only in the data browser, which means that you will not see the attribute in the record cards anymore. It also means that you will not be able to use attribute filters on that attribute. You could, however, define your data slices on an attribute, then hide it on the data browser, and your data slices will still work. This is best used for longer attributes that you don't want to clutter your data browser experience but are still informative during labeling.\n\n`Hide on labeling page`: This option will hide the attribute on both the data browser and labeling page. As these are the most interactive parts of the application, this is not that much different from hiding it completely. It will still appear on some other pages, e.g. in the \"run on 10\" feature of heuristics. This is best used for rather uninformative attributes that you don't need to be displayed at the moment, but are nice to see every once in a while in the application.\n\n`Hide`: This option will hide your attribute everywhere in refinery, where it does not negatively affect the user experience, e.g. it will still be visible in the \"copy attribute\" buttons of heuristics. Use this for attributes that you really don't need at the moment.\n<Note>\nChanging the visibility of an attribute is only temporary. You can always switch back to another visibility setting, so you can adapt this to the different requirements you might have at a certain point in your project.\n</Note>", "In refinery, sometimes you might want to run a code snippet for which there already is a brick module. To make it easier for you to know what brick modules are available to be imported in refinery, there is a bricks integration feature that helps you to import a brick module for your use case. The integrator asks you to assign a value to the user-defined variables or change the default values of the variables as needed. Once provided, the code is ready to be run in refinery.\n\nAn important thing to remember is that the bricks integration is highly dependent on the refinery version you might be using. If you are using an older version of refinery, chances are that you cannot access some of the brick modules which are only compatible with the newer version. The refinery version is visible on the bottom left panel of refinery.\n\nThis section provides a demonstration of using bricks in a refinery project using the bricks integration. For this documentation, we will use the \u201clanguage detection\u201d brick module.\n\nFor starters, we can create a new attribute on the settings page by clicking the `Add new attribute` button.\n\n<CaptionedImage src=\"/refinery/bricks-integration/1.png\" title=\"\"/>\nOnce you click `Add attribute` you will be redirected to this newly created attribute page, where the editor would look like this:\n<CaptionedImage src=\"/refinery/bricks-integration/2.png\" title=\"\"/>\nAs you can see, there are two options: `Search in bricks` and `Start from scratch` . The first option is the gateway to the bricks integration feature which opens a prompt where you can select any of the available bricks for your project, as shown below.\n<CaptionedImage src=\"/refinery/bricks-integration/3.png\" title=\"\"/>\nOnce you have selected the module to import, you can go through the overview of the module which contains information and a link that redirects you to that module on bricks. If you want to know what a specific brick does, or need an example, you can navigate to Input example which has a default input that is also imported from bricks, and you can modify it to perform test runs.\n<CaptionedImage src=\"/refinery/bricks-integration/4.png\" title=\"\"/>\nClicking the `Request Example` button generates the output that you expect to see for that default/user-defined input. Likewise, the `Reset to default` resets the prompt to its initial state.\n\nThe last part is the `Integration` feature which consists of fields that take user-defined values.\n<CaptionedImage src=\"/refinery/bricks-integration/5.png\" title=\"\"/>\nOnce you have the variables set, you can look through the final code and changes.\n<CaptionedImage src=\"/refinery/bricks-integration/6.png\" title=\"\"/>\nLastly, you can click on `Finish up` and the code from bricks will be imported to the editor on the attribute page, as shown here:\n<CaptionedImage src=\"/refinery/bricks-integration/7.png\" title=\"\"/>\nAnd you are set! To test it out, you can run the code on 10 samples by clicking the button below the code editor and the output shall be generated in no time!\n<CaptionedImage src=\"/refinery/bricks-integration/8.png\" title=\"\"/>\nBricks integration makes it more convenient for you to simply access the module without any hassles of finding the module specifically on the bricks app and worrying if the module is compatible with your current refinery version or not. Apart from that, the above example was for attribute calculation, for which only classifiers or generators are to be used. In a similar fashion, you can use the bricks integration while creating a new heuristic - be it a labeling function (classifier/extractor) or an active learner.", "When you are working with multiple people on one project, comments become super helpful to better communicate in an asynchronous manner. For example, during labeling the records, \nthere might be a record that you marked as \"business\" and your colleague marked as \"world\". This induces some ambiguity that can also be seen in the inter-annotator agreement on the [monitoring page](/refinery/monitoring). \nIn order to avoid confusions, you can add a comment explaining your decision and read other opinions of your colleagues. \nYou can create comments anywhere inside a project by clicking on the comments icon in the upper navbar (left of the notification center).\n\n<CaptionedImage src=\"/refinery/comments/comments-1.png\" title=\"Fig. 1: Screenshot of the labeling suite where the user chose to display the comment section on the right side.\"/>\n\nClicking the comments icon will open a sidebar, where you can see existing comments made by your colleagues. These are bound to project entities such as labeling tasks, data slices, or even on record level. \nComments are highly valuable e.g. if you want to set up a crowd labeling heuristic and want to help your annotators understand a labeling task.\n\nAn important thing to notice here is that the comments have restricted visibility. This means that some of the comments might get hidden when you navigate to a section of refinery where this comment might not be needed. A very good rule of thumb is: if you can interact with a certain entity on the page you are on, you will also see the related comments for it, e.g. comments on data slices are only visible on the monitoring page and the data browser as these are the only places where you'll interact with them.\n\nA visual example: in Fig. 1 you can see that the comment section has two comments, one for a labeling task and the other for a record.\nThe record comment has an instance associated with it, which in this case is the record with the `record_id` of 34. If you try to view the comments while being on a different record (`record_id` != 34), \nyou won't see it anymore. This visibility feature is important for refinery since otherwise your comments could become crowded really quickly.\n\n<CaptionedImage src=\"/refinery/comments/comments-2.png\" title=\"Fig. 2: Screenshot of the labeling suite with the opened comment section. The comment for the record with the 'running_id' of 34 is hidden as it is not the currently selected record anymore.\"/>\n\nThe comments panel is designed in such a way that you can always choose which side of the page you want to see the comments as it can sometimes happen that you're writing a comment, but want to refer to something obstructed by the comments sidebar. \n\n<CaptionedImage src=\"/refinery/comments/comments-3.png\" title=\"Fig. 3: Screenshot of the labeling suite with the comments sidebar on the left side.\"/>\n\nAs you can see in Fig. 3, the comments sidebar is now on the left side of the page which can be done by clicking on the arrow icon on the top left of the comments prompt.", "There are a few important global settings that can be set in the locally-hosted open-source version of refinery, which are all collected on the configuration page.\n\nThese settings include the available tokenizers and some boundaries for your project size, which we pre-configured based on our experiences with the application. Going beyond these recommended settings might worsen your overall experience with refinery, so make sure to back up everything beforehand so that in the worst-case scenario you can just reinstall it.\n\nTo change the configurations, head to the avatar icon and click on \"Change config\".\n<CaptionedImage src=\"/refinery/configuration-page/configuration_page_1.png\" title=\"Fig. 1: Screenshot of the home screen after installation. Access the config by clicking on your avatar in the top right corner of the screen.\"></CaptionedImage>\nThe settings are pretty self-explanatory. If you like refinery and don't mind us collecting a few usage statistics (just usage, nothing about your data!), we would kindly ask you to allow it here as it really helps in developing the product for your needs. If you want to turn it off, that is also fine, there are other ways to leave meaningful feedback, e.g. the [community discord channel](https://discord.gg/qf4rGCEphW) or the [discussion forum](https://github.com/code-kern-ai/refinery/discussions).\n<CaptionedImage src=\"/refinery/configuration-page/configuration_page_2.png\" title=\"Fig. 2: Screenshot of the global configuration page.\"></CaptionedImage>", "There are two ways of exporting your data that are useful for different purposes. There is the option to **download your records** and there is the option to **create a project snapshot**. Both are accessible on the settings page.\n\n<CaptionedImage src=\"/refinery/data-export/1.png\" title=\"Fig. 1: Screenshot of the settings page where the user is about to create a project snapshot.\"/>", "This option will be the most commonly chosen one when it comes to exporting your data in order to use it outside of refinery. Once you select to download your record, a modal will appear that lets you fully customize the data you want to export (see Fig. 2).\n\n<CaptionedImage src=\"/refinery/data-export/2.png\" title=\"Fig. 2: Modal that appears after clicking on the 'Download records' button.\"/>\nThe default settings are a great starting point, but the \"custom\" option on the top right makes everything selectable to your needs. There is also the option to export your data in the Label Studio format, which also comes with custom HTML code to get a refinery-like UI in Label Studio.\n\n<Note>\nAlways press the \"prepare download\" prior to downloading to get the latest data.\n</Note>\nOne of the most useful options is the \"export amount\" as you can select to export only a certain data slice. A few scenarios where this can help you in your data development process could be to e.g. only export labeled records, export training and testing data separately, or only export data that is above a certain weak supervision confidence threshold. \n\nYou don't have to decide that at exporting time, though. You can filter the data according to your needs afterward, so let's take a look at the exported file to better understand the schema:\n```json\n[\n    {\n        \"headline\": \"$200 Laptops Break a Business Model\",\n        \"running_id\": \"1479\",\n        \"__clickbait__MANUAL\": null,\n        \"__clickbait__WEAK_SUPERVISION\": \"yes\",\n        \"__clickbait__WEAK_SUPERVISION__confidence\": \"0.4685\",\n        \"headline__entities__MANUAL\": [\n            \"O\",\n            \"O\",\n            \"O\",\n            \"O\",\n            \"O\",\n            \"O\",\n            \"O\"\n        ],\n        \"headline__entities__WEAK_SUPERVISION\": [\n            \"MONEY\",\n            \"MONEY\",\n            \"O\",\n            \"O\",\n            \"O\",\n            \"O\",\n            \"O\"\n        ],\n        \"headline__entities__WEAK_SUPERVISION__confidence\": [\n            0.83,\n            0.83,\n            0.0,\n            0.0,\n            0.0,\n            0.0,\n            0.0\n        ]\n    }\n]\n```\n\n`headline` and `running_id` are the attributes of our records, they were given in the initial data import. Everything that starts with a double underscore `__` is a reference to a full attribute _classification_ labeling task, that comes with three separate entries:\n\n- `MANUAL`: the manually set label for this task.\n- `WEAK_SUPERVISION`: the weakly supervised label for this task.\n- `WEAK_SUPERVISION_CONFIDENCE`: the confidence for the weakly supervised label.\n\nThat means the JSON key `__clickbait__MANUAL` contains the manual label for the full record classification labeling task `clickbait` as a value.\n\nAs refinery also offers _information extraction_ labeling tasks or classification tasks that are defined on a single attribute and not on the whole record, you can also find other entries in the JSON, which follow the same pattern with three separate entries, but this time they are prefaced with the name of the attribute that the labeling task is defined on. Therefore, `headline__entities__MANUAL` refers to the manual label for the labeling task `entities` that is defined on the attribute `headline`. The labeling task type will define whether the entry is a list (extraction) or a single value (classification).\n\n If you need a refresher on the different types of labeling tasks, please have a look at the section [labeling tasks](/refinery/labeling-tasks).", "This data export option is mainly designed to be a backup option. If you want to export your whole refinery project that can be loaded back into refinery at a later point, then you should create a project snapshot.\n\n<CaptionedImage src=\"/refinery/data-export/3.png\" title=\"Fig. 3: Modal that appears after selecting the project snapshot creation.\"/>\nYou can customize the export to your needs (see Fig. 3), e.g. you could include the embedding tensors, which requires more disk space but saves on computation time when importing it back into refinery.", "The data browser is the heart of data management in refinery. With it, you can create labeling sessions, filter and inspect your data, find similar records, and much more. Let's dive right in!", "One of the core features of the data browser is its extensive filtering capabilities. The following section will explain what the different types of filters are, how they work, and how they can assist you in your labeling and data management tasks.", "Whether you want to search for textual patterns or just slice your data into chunks according to some attribute, the attribute filters yield a huge variety of combinations when you want to inspect and manage your data.\n\nAn attribute filter is a combination of an _attribute_, an _operator_, a _value_, a _selection state_, and _operator-specific options_. \n\nThe _attribute_ selection is a dropdown menu that displays all the available attributes of your data that are not actively hidden (see [attribute visibility](/refinery/attribute-visibility)). This selection will determine the attribute to which the filter is applied, which is the `headline` attribute in Fig. 1. \n\nThe _operator_ selection is also a dropdown, but with pre-configured selection options. We currently support the following options:\n- `EQUAL`: Checks for direct equality of an attribute and the provided value. Is case sensitive.\n- `BEGINS WITH`: Checks if the attribute starts with the provided value.\n- `ENDS WITH`: Checks if the attribute ends with the provided value. \n- `CONTAINS`: Checks if the provided value occurs anywhere in the attribute at least once.\n- `IN`: Similar to EQUAL, but you can provide a list of possible values instead of just one. Use the separator specified in the [configuration](/refinery/data-management#configuration) (default: \",\").\n- `IN WC`: Similar to IN, but you can use wildcard syntax in your value. The \"\\*\" and \"%\" match all characters and arbitrarily many of them, while the \"\\_\" and \"?\" match just a single character.\n- `GREATER`: Checks if the attribute is greater than the provided value. For text and category attributes this is done character by character, which you might know from sorting the names in a file system. For example \"a9\" > \"a10\",  because \"a\" = \"a\" and \"9\" > \"1\". |\n- `GREATER EQUAL`: Similar to GREATER, but also includes attributes that are equal to the provided value.\n- `LESS`: Checks if the attribute is smaller than the provided value. Text and category attributes are handled just like they are in GREATER.\n- `LESS EQUAL`: Similar to LESS, but also includes attributes that are equal to the provided value.\n- `BETWEEN`: Checks if the attribute is GREATER EQUAL than the first provided value and LESS EQUAL the second provided value.\n\nThe _value_ of an attribute filter is the user input that you give, e.g. `Falcons` in Fig. 1.\n\nThe _selection state_ of an attribute filter is the state of the checkbox on the very left. There are currently three different states: INCLUDE (blue checkbox), EXCLUDE (red checkbox), and IGNORE (empty checkbox). INCLUDE and EXCLUDE do not refer to the inclusion or exclusion of the filter itself, but rather if we want to include or exclude the records that fulfill our filter condition. If the selection state is set to IGNORE, the filter will not be applied. The selected state in Fig. 1 is INCLUDE.\n\nThe last part of an attribute filter are _operator-specific options_, which currently only is the option case sensitivity. We call them operator-specific because not all operators support them. Case sensitivity is supported by BEGINS WITH, ENDS WITH, and CONTAINS.\n\n<CaptionedImage src=\"/refinery/data-management/1.png\" title=\"Fig. 1: Screenshot of the data browser where a single attribute filter is set. The matched filtering criterium is highlighted in yellow (see headline attribute).\"/>\nIn Fig. 1 you can see an example of an attribute filter. Here, the attribute is `headline`, the operator is `BEGINS WITH`, the value is `Falcons`, the selection state is `INCLUDE` (blue checkbox) and the operator-specific option \"Case sensitive\" is not selected.\n<Note>\nWant to apply multiple filters on different attributes at the same time?\nJust click on the \"+\"-icon below your attribute filter to add a new attribute filter. Multiple filters are combined with AND, so you are filtering for the intersection of records, where all the attribute filters evaluate to true.\n</Note>", "Each labeling task that you create will automatically get its own collapse filter menu in the data browser. Every labeling task filter comes with the following options: _manually labeled_, _weakly supervised_, _model callback_, and _heuristics_.\n\nThe _manually labeled_ multi-select dropdown lets you filter your data based on the manual label that was given for that specific labeling task. The multi-select dropdown displays all available labels (even if they weren't assigned to any record) and one additional option \"has no label\". The label selection also has three states: INCLUDE (blue checkbox), EXCLUDE (red checkbox), and IGNORE (empty checkbox). Per default, everything is set to IGNORE, which is why nothing is filtered. The \"has no label\" option is a great way of filtering your data for unlabeled records and only has the states INCLUDE and IGNORE.\n\nThe _weakly supervised_ multi-select dropdown lets you filter your data based on the [weak supervision](/refinery/weak-supervision) label that was given for that specific labeling task. It functions just like the _manually labeled_ one with the addition of a confidence interval. You can use the confidence interval filtering without actually filtering for any specific weak supervision label.\n\nThe _model callback_ multi-select dropdown lets you filter your data based on the [model callback](/refinery/model-callbacks) labels that you added to your project. They work similarly to the weakly supervised filter option.\n\n_Heuristics_ filtering option works a little differently from the others because it cannot directly filter for a label. Instead, you can select a set of heuristics and only show those records that the heuristics make a prediction on. This filter also has three states INCLUDE, EXCLUDE, and IGNORE. The filter will show all available heuristics, even if they were not executed yet. There is a special selection option called \"only with different results\", which will filter for the records where at least two heuristics disagree in their prediction.\n\n<CaptionedImage src=\"/refinery/data-management/2.png\" title=\"Fig. 2: Screenshot of the data browser where the user filtered for the classification labeling task 'topic'. This filter setting (selected heuristics and no manual label) is great for validating heuristics.\"/>", "The comment filter is very basic. It has one option, to filter for \"records with comments\". This also has three states INCLUDE, EXCLUDE and IGNORE, so you can filter for records that have comments, have no comments, or any of the two.", "Filtering options can be combined arbitrarily. The combination operator of different filter settings is the logical AND, meaning that if you select multiple filters, you will receive the intersection of records that fulfill all the filtering criteria.\n\nAt the bottom of the filter options, you will find a radio selection that is prefaced with the words \"connect by\". This selection influences the combination operator **only within one filter**. That means the general operator will still be the AND (intersection), but atomic filter options can change their operator to be concatenated with OR (union). At this moment, the \"connect by\" selection changes the behavior of the following filter options: _users_, _manually labeled_, _weakly supervised_, _model callback_, and finally _heuristics_.\n\n<CaptionedImage src=\"/refinery/data-management/3.png\" title=\"Fig. 3: Screenshot of the data browser where the user filtered for non-labeled records that mention Germany and have conflicting predictions of heuristics.\"/>\n<Note>\nFilter combinations are not saved automatically. When you filter your data and want to start a labeling session or navigate to another page, your filter settings will not be saved! If you want to persist these settings, please save them as a [data slice](/refinery/data-management#from-filters-to-data-slices).\n</Note>", "As filters are really useful to find patterns in your data, distribute labeling work, or validate heuristics, you naturally want to save them for later access. When you save a filter setting, you create a slice of your data that fulfills these filtering criteria, which we simply call a \"data slice\". This section will give you a comprehensive overview of the different types of slices and how they work in refinery.", "Once you selected your filtering criteria, you can create a data slice by pressing the \"save filter\" button on the bottom of the filter sidebar. After selecting to save the filter, a modal will appear that requires two inputs: the type of the data slice, and a unique name for it. Finally, press \"save filter\" and the new data slice should appear at the top of the filter sidebar.\n\nThere is a little color-coded information icon next to every data slice to indicate its type. You can also click on it for more information about the data slice, like full name, creation date, creator, and type. If you created a static data slice on the [managed version](/refinery/managed-version) of refinery, you can also view this slice as if you had the role of an expert to validate that this is something that you want to hand to an actual expert for labeling. \n\n<CaptionedImage src=\"/refinery/data-management/4.gif\" title=\"Fig. 4: Short GIF that shows how to create a data slice in the data browser.\"/>", "There are two types of data slices that you might want to consider for different use cases: the _static_ and the _dynamic_ data slice.\n\nThe _dynamic_ data slice only saves the filter settings that were used to create it. It does not have a state and therefore the filters are re-applied to your data every time you work with this data slice. So if you want to have your data slice up-to-date at all times, go with this option.\n\nThe _static_ data slice on the other hand saves not only the filter configuration but also the records (or rather their indices) that are fulfilling the filtering criteria at the time of creation. When you re-select the data slice later in your project, only the saved records will be displayed even if they might not match the current filter criteria anymore. If there is, however, a difference in _the number of records_ that were saved during creation and the number of records that would fulfill the filtering criteria at the time of selection, there will be a small triangle warning indication above the data slices at the top of the filter sidebar as well as an asterisk next to the count above the individual records. At that time, you can select to update your slice and it will then save the records that are fulfilling the filtering criteria at that moment.\n\nIf you want to [monitor](managing-roles) your data on specific data slices, you must use _static_ data slices.\n<Note>\nFor efficiency reasons, the number of saved records in a static data slice is currently capped at 10,000.\n</Note>\n<CaptionedImage src=\"/refinery/data-management/5.png\" title=\"Fig. 5: Screenshot of the data browser that currently holds a static (orange) and a dynamic (blue) data slice.\"/>", "<Note>\nThis feature is reserved for the multi-user managed version of refinery.\n</Note>\nEach static filter has a URL attached to it, which you can send to colleagues that have the expert or annotator role. Simply click on the info icon, click on the URL to copy it to your clipboard, and finally send it to your colleague. They will directly start in the expert labeling view after logging in.\n\nIf you need a refresher on the role system in refinery, head over to the [role management page](/refinery/managing-roles) in this documentation.", "Every uploaded or usable attribute of your data that is not hidden will be available for ordering the results in the data browser. Additionally, you have the option to order by weak supervision confidence, model callback confidence, and by random (reproducible thanks to the seed).\n\nYou can order your results ascending or descending and there is no nested or combined ordering, so you have to select a single attribute or option to order by.\n\n<CaptionedImage src=\"/refinery/data-management/6.png\" title=\"Fig. 6: Screenshot of the data browser where the user filtered for records that have a weak supervision label and no manual label. The records are ordered by ascending weak supervision confidence to identify hard classification examples for manual labeling.\"/>", "The data browser integrates embedding-based outlier detection and similarity search. For full documentation on these features, please visit the [neural search](/refinery/neural-search) page.", "There are a few extra settings that would have been too much on the eye if we incorporated them directly into the UI, which is why they are collected under \"configuration\" on the top right of the data browser. \n\n_Highlight text_ (default enabled): When using attribute filters, you have the option to highlight the matching search terms in your records. This highlighting is not available for comparison operators like BETWEEN, GREATER, and LESS.\n\n_Only show weakly supervised-related_ (default disabled): This setting decides if the data browser shows all predictions from heuristics for each individual record or only those, that went into calculating the current weak supervision label. \n\n_IN operator selection_ (default \",\"): This setting determines which separator you use for the IN operator of the attribute filter and is mostly only needed if you want to search for something that contains the current separator, in which case you can switch it. Available options are a comma and a dash.", "This is a small collection of best practices that we found useful in a labeling project. We will expand this section with time, so check it every once in a while for new inspiration!", "You can also use the labeling-task-specific drawers to select for potential labeling mismatches. As you can order by the weak supervision confidence score, this makes it easy to either find manual labeling errors (i.e. there is a mismatch and the weakly supervised label has a high likelihood) or weak supervision bugs.\n<CaptionedImage src=\"/refinery/data-management/7.png\" title=\"Fig. 7: Screenshot of the data browser where the user is looking for label mismatches between weak supervision and the manual label.\"/>", "Depending on your task, one of the first things you can do is to pick one (or multiple) embeddings for your data. If you're not familiar with embeddings yet, make sure to take a look at [our blog](https://www.kern.ai/company/blog) or check out other resources like [this](https://www.kdnuggets.com/2021/11/guide-word-embedding-techniques-nlp.html) one.", "To create one, click \"Generate embedding\" on the project settings page. A modal will open up, asking you for the following information:\n\n- `target attribute`: If you have multiple textual attributes, it makes sense to compute embeddings for them step-by-step. Here you can choose the attribute you want to encode.\n- `granularity`: You can both calculate the embeddings on the whole attribute (e.g. a sentence) or for each token. The latter option is helpful for extraction tasks, whereas attribute embeddings help you both for classification tasks and [neural search](/refinery/neural-search). We recommend that you always begin with attribute-level embeddings.\n- `configuration`: This defines the model to use. You can choose from the recommended options, or type in any configuration string from Hugging Face.\n<CaptionedImage src=\"/refinery/embedding-integration/embedding_integration_1.png\" title=\"Fig. 1: Screenshot of the project settings page where the user adds a new embedding. The options with green text are already downloaded models.\"/>\nOn the managed version, the embedding creation is calculated on a GPU-accelerated instance. Generally, this process might take some time, so it might be time to grab a hot coffee. \u2615\n\n<Note>\nTo save you disk space, the embeddings are reduced to 64 dimensions using PCA.\n In our experiments, this had no significant impact on the performance of neural search, active learners, or any other embedding-related feature of refinery. You can read more about it [here](https://www.sbert.net/examples/training/distillation/README.html#dimensionality-reduction).\n</Note>\n\nOnce the computation is finished, the embedding is usable for active learning (and in the case of attribute-level embeddings, for [neural search](/refinery/neural-search)).", "If you are using the managed version of refinery, you can download commonly used models so that they don't have to be pulled from huggingface every time you are using them. They are then globally available to your workspace, not only for a single project.\n\nIn order to do that, go into any project settings page and find the \"see downloaded models\" button beneath the embeddings. Just add a new model there. The whole process is shown in Fig. 2.\n\n<CaptionedImage src=\"/refinery/embedding-integration/embedding_integration_2.gif\" title=\"Fig. 2: GIF of the user adding a huggingface model to the downloaded models.\"/>", "Evaluating your heuristics is key in making informed decisions on their inclusion or exclusion during weak supervision. Even though there are other indicators of heuristic quality, the most important one is the estimated accuracy, which gets more and more accurate the more manually labeled reference data you have. So before committing fully to evaluating your heuristics, first make sure to provide enough manually labeled data!", "Refinery constantly keeps track of how well your heuristics are doing, no matter what type they are. Once you execute a heuristic - and there is some manually labeled data we can use for evaluation - you will find the following statistic at the bottom of your heuristic details page:\n\n- _Est. precision_: estimated precision on the manually labeled reference data (records that have the same label as the heuristic is assigning). Calculated with `true positives / (true positives + false positives)`. Gets more precise the more reference data you label.\n- _Coverage_: Number of records that this heuristic makes a prediction for.\n- _Hits_: Number of spans that this heuristic makes a prediction for. This will always be the same number as coverage for classification tasks but could differ for extraction tasks as they could have multiple spans labeled for a single record.\n- _Conflicts_: Number of instances (record or span) where this heuristics's prediction is different from other heuristic's predictions. Is by definition smaller or equal to overlaps.\n- _Overlaps_: Number of instances (record or span) where this heuristic makes a prediction and at least one other heuristic also makes a prediction.\n<CaptionedImage src=\"/refinery/evaluating-heuristics/1.png\" title=\"Fig. 1: Screenshot of the statics on the details page of an active learner.\"/>", "As statics are crucial to the estimation of heuristic quality, refinery updates them automatically and the user must not re-run them all the time. Because updating the statistics after every manual label would be computationally expensive, we opted for a debounce time. This means that after you labeled a little bit of data and have not set any new label for a few seconds, refinery triggers a re-run of the statistics calculation in the background. That means they will not be available immediately but will reach consistency eventually (just a few seconds usually).", "When working with heuristics in refinery, you develop certain patterns of evaluation that can be regarded as best practices. We want to share a few of those here and will constantly expand this section.", "We know this does not sound exciting, but one of the best (unbiased) ways of validating your heuristics is to label data randomly. Just go into the data browser, select the manual label filter option \"has no label\" for your respective labeling task, go to the result order and select \"random\". Either save this a dynamic data slice or just jump into the labeling session.", "One common way to precisely validate a single heuristic is to create a dynamic data slice with two criteria:\n\n- no manual label\n- heuristic makes a prediction\n\nThis data slice will always contain data that is relevant to give a better precision estimate as you provide more manually labeled reference data for the statistics. Usually, labeling 15-30 records directly after creating the heuristic already gives a good estimate.", " ", "Labeling functions are the simplest concept of a heuristic. They consist of some Python code that labels the data according to some logic that the user can provide. This can be seen as transferring the domain expertise from concepts stored in a person's mind to explicit rules, which are human- and machine-readable.", "To create a labeling function simply navigate to the heuristics page and select \"Labeling function\" from the \"New heuristic\" button.\n<CaptionedImage src=\"labeling-functions/labeling_functions_1.png\" title=\"Fig. 1: Screenshot of the heuristics page where the user is about to add a labeling function by accessing the dropdown at the top.\"/>\nAfter that, a modal will appear where you need to select the labeling task that this heuristic is for and give the labeling function a unique name and an optional description. These selections are not finite and can be changed easily later.\n<CaptionedImage src=\"labeling-functions/labeling_functions_2.png\" title=\"Fig. 2: The user selected the classification labeling task 'topic' for this labeling function, which will be a simple lookup function.\"/>\nAfter creation, you will be redirected to the details page of that labeling function, which is also accessible from the heuristics overview. Here, you have the option to start your labeling function from scratch or to \"search in bricks\". [Bricks](https://github.com/code-kern-ai/bricks) is our open-source content library that collects a lot of standard NLP tasks, e.g. language detection, sentiment analysis, or profanity detection. If you are interested in integrating a bricks module, please look at the [bricks integration](/refinery/bricks-integration) page. \n<CaptionedImage src=\"labeling-functions/labeling_functions_3.png\" title=\"Fig. 3: Screenshot of the labeling function details page after initialization.\"/>\nThe next section will show you how to write a labeling function from scratch.", "After creation, you will be redirected to the details page, which packs many features. For this section, we will be concentrating on the code editor.\n\nBefore we start coding, let's talk about the signature of the labeling function:\n\n- The input `record` is a dictionary containing a single record with the attribute names as keys\n  - as we tokenize _text_ attributes, they won't be a simple string, but rather a [spaCy Doc object](https://spacy.io/api/doc) (use `attribute.text` for the raw string)\n  - the other attributes can be accessed directly, even _categorical_ attributes\n- The output will be different depending on the type of labeling task\n  - a classification task must have a `return` statement that returns an existing label name as a string\n  - an extraction task must have a `yield` statement that follows the pattern of `yield YOUR_LABEL, span.start, span.end` where `YOUR_LABEL` also is an existing label name as a string\n\nTo write a labeling function, you just input your code into the code editor. Be aware that **auto-save is always on**! So if you plan to make big changes, either create a new labeling function or save the old code in a notepad.\n\n<CaptionedImage src=\"labeling-functions/labeling_functions_4.png\" title=\"Fig. 4: Screenshot of the labeling function details page where the user finished writing the labeling function. It imports a lookup list called 'business' and looks for any of these terms in the headline attribute in order to return the label 'business'. This could be extended to look for a minimum threshold of matching terms to increase accuracy.\"/>\nAbove the code editor, there are some quality-of-life features:\n\n- _Attributes_: list of attributes that are available in your data. The colors of these buttons indicate the data type, hover over them for more details. They can also be pressed which copies the name to your clipboard - no more typos!\n- _Lookup lists_: list of available lookup lists and the number of terms that are in them. Click them to copy the whole import statement to the clipboard.\n- _Editor_: the dropdown right next to it defines the labeling task that the labeling function will be run on. Next to that are more colorful buttons that represent the available labels for that task. Click them to copy to the clipboard!\n<Note>\nThere are many [pre-installed useful libraries](https://github.com/code-kern-ai/refinery-lf-exec-env/blob/dev/requirements.txt), e.g. beautifulsoup4, nltk, spacy, and requests. You can check what libraries are installed by clicking the `See installed libraries` button on the top right corner just above the editor. You read that right, `requests` work within labeling functions! So you can also call outside APIs and save those predictions as a labeling function. Though, if you have a production model that you want to incorporate, we suggest using [model callbacks](/refinery/model-callbacks).\n</Note>\n\nOur labeling function is still in the state `initial`, which means we cannot use it anywhere. To change that, the next chapter will cover running this function on your data.", "After you've written your labeling function, you have three options to run it on your data:\n\n- _Run on 10_: Randomly sample 10 records from your data and run the selected function on it, then display the selected attribute of the record with the prediction of the labeling function.\n- _Run_: Run the function on all the records of your project. This will alter the state of your labeling function to `running` and after that (depending on the outcome) either to `finished` or `error`.\n- _Run + weakly supervise_: Just like _Run_ but triggers a weak supervision calculation with all heuristics selected directly afterward.\n<CaptionedImage src=\"labeling-functions/labeling_functions_5.png\" title=\"Fig. 5: Screenshot of the labeling function details page where the user tested his labeling function called lookup_business on 10 randomly selected records with the 'run on 10' feature. The user can now observe what records are predicted to be 'business'-related to validate the function.\"/>\nWe generally recommend first running your function a bunch of times with the \"run on 10\" feature, as you can observe edge cases, bugs, and other things while doing so.\n\n<CaptionedImage src=\"labeling-functions/labeling_functions_6.png\" title=\"Fig. 6: Screenshot of the labeling function details page after running the labeling function on the full data (indicated by the state finished).\"/>\nYour code is run as isolated containerized functions, so don't worry about breaking stuff. This is also the reason why we display \"container logs\" below the code editor. For all the information on these logs, go to the container logs section further down on this page.", "Deleting a labeling function will remove all data associated with it, which includes the heuristic and all the predictions it has made. However, it does not reset the weak supervision predictions. So after deleting a labeling function that was included in the latest weak supervision run, the predictions are still included in the weak supervision labels. Consider re-calculating the weak supervision if that is an issue.\n\nIn order to delete the labeling function simply scroll to the very bottom of the labeling function details, click on the delete button, and confirm the deletion in the appearing modal.\n\nAlternatively, you can delete any heuristic on the heuristics overview page by selecting it and going to Actions -> delete selected right next to the weak supervision button. Don't worry, there will still be a confirmation modal.", "When you run (or run on 10) your labeling function, refinery executes it in a freshly spawned docker container, which you cannot inspect easily from outside as it shuts itself down after the calculation finishes. That is why we display the resulting logs of the execution on the labeling function details page.\n\nThe logs of the latest full run are persisted in the database while the logs of the \"run on 10\" feature are only cached in the frontend, which means you will lose them as soon as you refresh or leave the page.\n<CaptionedImage src=\"labeling-functions/labeling_functions_7.png\" title=\"Fig. 7: Screenshot of the labeling function details page where the user added a print statement to the labeling function to observe which terms matched with his records.\"/>\nBesides inspecting error messages, you can also use the container logs for debugging your labeling function using print statements.\n<CaptionedImage src=\"labeling-functions/labeling_functions_8.png\" title=\"Fig. 8: Screenshot of the labeling function details page where the user ran his labeling function with the print statement (from Fig. 7) on 10 randomly sampled records. You can see the print results in the container logs.\"/>", " ", "After running your labeling function on the whole dataset, you get statistics describing its performance. The statistics should be used as an indicator of the quality of the labeling function. Make sure to understand those statistics and follow best practices in [evaluating heuristics](/refinery/evaluating-heuristics).", "You'll quickly see that many of the functions you want to write are based on list expressions. But hey, you most certainly don't want to start maintaining a long list in your heuristic, right? That's why we've integrated automated lookup lists into our application.\n\nAs you manually label spans for your extraction tasks, we collect and store these values in a lookup list for the given label.\n<CaptionedImage src=\"labeling-functions/labeling_functions_9.png\" title=\"Fig. 9: Screenshot of the labeling suite where the user labels a span that is automatically added to a lookup list.\"/>\nYou can access them via the heuristic overview page when you click on \"Lookup lists\". You'll then find another overview page with the lookup lists.\n<CaptionedImage src=\"labeling-functions/labeling_functions_10.png\" title=\"Fig. 10: Screenshot of the lookup lists overview accessed by going through the heuristic overview page.\"/>\nIf you click on \"Details\", you'll see the respective list and its terms. You can of course also create them fully manually, and add terms as you like. This is also helpful if you have a long list of regular expressions you want to check for your heuristics. You can also see the python variable for the lookup list, as in this example `country`.\n<CaptionedImage src=\"labeling-functions/labeling_functions_11.png\" title=\"Fig. 11: Screenshot of the lookup list details view where one can manage the lookup list.\"/>\nIn your labeling function, you can then import it from the module `knowledge`, where we store your lookup lists. In this example, it would look as follows:\n<CaptionedImage src=\"labeling-functions/labeling_functions_12.png\" title=\"Fig. 12: Screenshot of a labeling function importing and utilizing a lookup list called 'country'.\"/>", "You might already wonder what labeling functions look like for extraction tasks, as labels are on token-level. Essentially, they differ in two characteristics:\n\n- you use `yield` instead of `return`, as there can be multiple instances of a label in one text (e.g. multiple people)\n- you specify not only the label name but also the start index and end index of the span.\n\nAn example that incorporates an existing knowledge base to find further examples of this label type looks as follows:\n<CaptionedImage src=\"labeling-functions/labeling_functions_13.png\" title=\"Fig. 13: Screenshot of a labeling function for an extraction task.\"/>\n\nThis is also where the tokenization via `spaCy` comes in handy. You can access attributes such as `noun_chunks` from your attributes, which show you the very spans you want to label in many cases. Our [template functions repository](https://github.com/code-kern-ai/template-functions) contains some great examples of how to use that.", "Active learners are few-shot learning models that leverage the powerful pre-trained language models that created the embeddings of your records. You can treat them as custom classification heads that are trained on your labeled reference data. They are different from the classical notion of an active learner in the sense that they do not query the user for data points to be labeled, but more on that in the best practices section.\n\nMany concepts in the UI are similar to the ones of the [labeling functions](/refinery/heuristics#labeling-functions).", "To create a labeling function simply navigate to the heuristics page and select \"Active learning\" from the \"New heuristic\" button.\n\n<CaptionedImage src=\"active-learners/active_learners_1.png\" title=\"Fig. 1: Screenshot of the heuristics overview page where the user is about to create a new active learner.\"/>\nAfter that, a modal will appear where you need to select the labeling task that this heuristic is for and give the active learner a unique name and an optional description. These selections are not finite and can be changed later, but they are used for the initial code generation, which will be a lot easier to use if you select the right things.\n<CaptionedImage src=\"active-learners/active_learners_2.png\" title=\"Fig. 2: Screenshot of the heuristics overview page where the user entered all the information required to create a new active learner.\"/>\nAfter creation, you will be redirected to the details page (Fig. 3) of that active learner, which is also accessible from the heuristics overview. There is a lot of pre-filled code, which will be explained in the next section, where we write an active learner.\n\n<CaptionedImage src=\"active-learners/active_learners_3.png\" title=\"Fig. 3: Screenshot of the active learner details page after creation. The user has not modified anything, the code is entirely pre-filled.\"/>", "When you create an active learner, there will be a lot of pre-filled code, which is a good baseline to work with, that you should keep as a structure and only exchange parts of. If you are not interested in the technical details and want to use active learners without touching code too much, please skip forward to the best practices and examples.\n\nGenerally, you have to define a new class that carries the name of the active learner, which implements an abstract `LearningClassifier` (see Fig. 3). In order to use this active learner, you have to implement three abstract functions:\n\n- `__init__(self)`: initializes the `self.model` with an object that fulfills [sklearn estimator interface](https://scikit-learn.org/stable/developers/develop.html). **Can be modified to use the model of your choice.**\n- `fit(self, embeddings, labels)`: fits `self.model` to the embeddings and labels. **Should not be modified.**\n- `predict_proba(self, embeddings)`: makes predictions with `self.model` and returns the probabilities of those predictions. **Should not be modified.**\n\nIf you are wondering why you should not modify two of those functions, it is because we put all the interchangeable parameters into the decorators (which begin with an \"@\"). \n\nIn `@params_fit` you can specify the embedding name and train-test-split for the fitting process of your model. All valid embedding names can be seen above the code editor and the specified embedding will also be the one used for prediction. The train-test-split is currently fixed to a 50/50 split to ensure that there are enough records for thorough validation.\n\nIn `@params_inference` you can specify the minimal confidence that your model should have in order to output the prediction it made and the label names which this active learner should make predictions for. One active learner can be trained on any amount of classes. If you specify `None`, then all the available labels are used. The default value for `min_confidence`is 0.9, which makes sure that your predictions are less noisy. If you find to suffer from very low coverage, consider changing this value, but the lower you set it, the more validation data should be available.\n\nPer default, we use the `LogisticRegression` from sklearn, which is common practice and we recommend trying it before switching to another model.", "After you have finished writing your active learner (or just went with the default one), you can run it using either:\n\n- _Run:_ Fits the active learner on all manually labeled records from the training split that carry a label specified in `label_names`. After the fitting, it will make predictions for all records while obeying the specified `min_confidence`. Finally, it will calculate the statistics using the test records. This action will alter the state of your active learner first to _running_ and after that (depending on the outcome) either to _finished_ or _error_.\n- _Run + weakly supervise:_ Just like Run but triggers a weak supervision calculation with all heuristics selected directly afterward.\n\n<CaptionedImage src=\"active-learners/active_learners_4.png\" title=\"Fig. 4: Screenshot of the active learner details page after running it. The combination of extreme precision estimates and low coverages suggests that the user should try a lower confidence threshold.\"/>\nFor more details on the statistics, please visit the page about [evaluating heuristics](/refinery/evaluating-heuristics).", "Deleting an active learner will remove all data associated with it, which includes the heuristic and all the predictions it has made. However, it does not reset the weak supervision predictions. So after deleting an active learner that was included in the latest weak supervision run, the predictions are still included in the weak supervision labels. Consider re-calculating the weak supervision if that is an issue.\n\nIn order to delete the active learner simply scroll to the very bottom of the active learner details, click on the delete button, and confirm the deletion in the appearing modal.\n\nAlternatively, you can delete any heuristic on the heuristics overview page by selecting it and going to \"Actions\" -> \"delete selected\" right next to the weak supervision button. Don't worry, there will still be a confirmation modal.", " ", "After running your active learner on the whole dataset, you get statistics describing its performance. The statistics should be used as an indicator of the quality of the active learner. Make sure to understand those statistics and follow best practices in [evaluating heuristics](/refinery/evaluating-heuristics).", "You can use [Scikit-Learn](https://scikit-learn.org/stable/) inside the editor as you like, e.g. to extend your model with grid search. The `self.model` is any model that fits the [Scikit-Learn estimator interface](https://scikit-learn.org/stable/developers/develop.html), i.e. you can also write code like this:\n```python\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\n\nclass ActiveDecisionTree(LearningClassifier):\n\n    def __init__(self):\n        params = {\n            \"criterion\": [\"gini\", \"entropy\"],\n            \"max_depth\": [5, 10, None]\n        }\n        self.model = GridSearchCV(DecisionTreeClassifier(), params, cv=3)", "```\nAs with any other heuristic, your function will automatically and continuously be [evaluated](/refinery/evaluating-heuristics) against the data you label manually.", "One way to improve the precision of your heuristics is to label more data (also, there typically is a steep learning curve, in the beginning, so make sure to label at least some records). Another way is to increase the `min_confidence` threshold of the `@params_inference` decorator. Generally, precision beats recall in active learners for weak supervision, so it is perfectly fine to choose higher values for the minimum confidence.", "We're using our own library [sequencelearn](https://github.com/code-kern-ai/sequence-learn) to enable a Scikit-Learn-like API for programming span predictors, which you can also use [outside of our application](https://github.com/code-kern-ai/sequence-learn/blob/main/tutorials/Learning%20to%20predict%20named%20entities.ipynb).\n\nOther than importing a different library, the logic works analog to active learning classifiers.", "Zero-Shot Classification refers to the task of predicting a class that wasn't seen by the model during training. In refinery, this is accomplished by embedding the label names and text attributes into the same space, where distance metrics (e.g. cosine similarity) can be used to assign the closest label to the respective record. Read more about zero-shot classification [here](https://huggingface.co/tasks/zero-shot-classification).\n<Note>\nZero-shot classifiers are only available for multiclass classification. We are still in the process of investigating the capabilities of zero-shot classifiers for extraction tasks.\n</Note>", "Just like the other heuristics, we can add a zero-shot classifier by visiting the heuristic overview page and selecting \"zero-shot\" in the \"new heuristic\" dropdown menu.\n<CaptionedImage src=\"zeroshot-classifier/zeroshot_classifier_1.png\" title=\"Fig. 1: Screenshot of the heuristic overview page where the user is creating a new zero-shot classifier.\"/>\nThe information we then have to fill out (see Fig. 2) is pretty straightforward. Select the task and attribute, then input a handle from Hugging Face. Make sure that the Hugging Face model is suitable for zero-shot classification. You can browse the available models [here](https://huggingface.co/models?pipeline_tag=zero-shot-classification).\n<CaptionedImage src=\"zeroshot-classifier/zeroshot_classifier_2.png\" title=\"Fig. 2: Screenshot of the zero-shot classifier creation dialogue.\"/>\nAfter creation, you are redirected to the details page of the zero-shot classifier (see Fig. 3).\n<CaptionedImage src=\"zeroshot-classifier/zeroshot_classifier_3.png\" title=\"Fig. 3: Screenshot of the zero-shot classifier details page.\"/>", "In theory, you could run this classifier out of the box with the default configuration by just clicking on the \"run\" button. But there are a few things one should keep in mind for optimal performance. As this classifier is really relying on the wording of the labels for classification, you should first make sure that the label names are expressive. If a label is called `yes`, then it does not carry any information for this zero-shot module even though it might totally make sense for a human labeler in combination with the name of the labeling task. For example, consider renaming `yes` to `clickbait headline` if it is a `clickbait` labeling task.\n\nBecause this module requires a little experimentation, refinery gives you the tools to try out a lot of variations. The first thing is the \"run on 10\" feature, which is common among the heuristics. It samples 10 random records from your data and executes the classifier on them.\n<CaptionedImage src=\"zeroshot-classifier/zeroshot_classifier_4.png\" title=\"Fig. 4: Screenshot of the zero-shot classifier details page, where the user ran it on 10 randomly sampled records of the `AG News` sample use case.\"/>\nAfter running the classifier on 10 samples, we can inspect the results in the table below (see Fig. 4). All the yellow exclamation marks indicate that these records are below the prediction threshold that was set further up on this page (a good default is 40 %). \n\nWhen inspecting these results (Fig. 4), we can see that the first entry was predicted to carry the label `world`. If we read the headline carefully, we might get an idea why that could have been:\n\n> Gamers Eye Open Virtual Worlds Fans of open-source programming and multiplayer gaming explore ways to bring the concept of the metaverse -- an infinitely expandable virtual `world` -- closer to fruition. By Daniel Terdiman.\n\nIf we now click on the \"view\" button next to the label, we get the full details of that record and prediction (see Fig. 5). The classifier lists `sci_tech` as the second most probably option, which would be the correct label, but apparently `sci_tech` is not expressive enough (overall, it is not even a real world) and `world` expresses the wrong concept, even though we humans might be able to use a better abstraction of it.\n\n<CaptionedImage src=\"zeroshot-classifier/zeroshot_classifier_5.png\" title=\"Fig. 5: Screenshot of the detailed predictions after the user clicked on the 'view' button.\" />\nIn order to prototype better label descriptions, we can use the zero-shot playground on this details page. For that, we just insert the text we want to classify into the first textbox and put the new label names into the textbox below (comma separated). We then press \"compute example\", which returns the probabilities of the new label names. This has been done in Fig. 6, where the user replaced `sci_tech` with `science and technology` and replaced `world` with `international news`. The assigned label is now the correct one with a better-looking probability distribution.\n<CaptionedImage src=\"zeroshot-classifier/zeroshot_classifier_6.png\" title=\"Fig. 6: Screenshot of the zero-shot classifier details page where the user experiments with new label names to improve zero-shot performance.\"/>\nIf you like the new label names and want to test them on more data, you can now use the \"run on 10\" feature with these new label names. Just leave them entered in the textbox and press \"run on 10\". This way you get a better overview, which helps you further identify potential issues. But don't get lost in this iterative process. In the end, the zero-shot classifier is just a single imperfect heuristic in a whole collection of other heuristics.\n\n<CaptionedImage src=\"zeroshot-classifier/zeroshot_classifier_7.png\" title=\"Fig. 7: Screenshot of the zero-shot classifier details page where the user utilizes the 'run on 10' feature with custom label names to test them on more data before finalizing the renaming.\"/>", "Running the zero-shot classifier will only consider those labels whose boxes are checked at the top of the details page. So if you need to make final adjustments to the names of your labels, you need to head over to the settings page and change them before running this heuristic.\n\nIf you want to exclude some labels, because they are used more as flags instead of labels (e.g. `can be deleted?` or `needs clarification`), then make sure that they are unchecked before running the heuristic.\n\nFinally, if everything is ready, you just have to click on \"run\" and the heuristic will be executed on all of your records. All predictions that don't surpass the set threshold are dropped.", "You can delete the zero-shot classifier by scrolling to the very bottom of its details page and clicking on the delete button. This will remove all predictions of this heuristic, but won't change the weak supervision results that were calculated prior to the deletion.\n\nAlternatively, you could go to the heuristics overview page, select the classifier, and then press on \"actions\" -> \"delete selected\". This will result in the same action.", " ", "After running your zero-shot classifier on the whole dataset, you get statistics describing its performance. The statistics should be used as an indicator of the quality of the classifier. Make sure to understand those statistics and follow best practices in [evaluating heuristics](/refinery/evaluating-heuristics).", "<Note>\nThis feature is only available in the [managed version](/refinery/managed-version).\n</Note>\nWhen you have some annotation budget available or just know some colleagues who have a little less domain expertise than required, you can still incorporate their knowledge as a crowd labeling heuristic. Before starting with this, you should be aware of the implications of this design choice. As this is only a heuristic, you will not be able to use these labels individually, but only in an aggregated form by incorporating them in the [weak supervision](/refinery/weak-supervision) process.", "Before you can use the crowd labeling heuristic, you should first create static data slices that the annotators will work on. If you want to randomly distribute work, we suggest slicing your data according to the primary key (e.g. `running_id`), shuffling it randomly, and saving that to a slice.\n\nYou should also have at least one user with the annotator role assigned to your workspace. If that is not the case, get in contact with us and we will set it up together with you.", "As with every heuristic, we start by visiting the heuristics page and selecting \"crowd labeling\" in the dropdown \"new heuristic\".\n<CaptionedImage src=\"crowd-labeling/crowd_labeling_1.png\" title=\"Fig. 1: Screenshot of the heuristics overview page. The user is adding a new crowd labeling heuristic.\"/>\nThis heuristic also requires the user to specify the labeling task, a name, and a description. This can also still be updated in the next step.\n<CaptionedImage src=\"crowd-labeling/crowd_labeling_2.png\" title=\"Fig. 2: Screenshot of the modal that appears after selecting to add a new crowd labeling heuristic.\"/>\nAfter you entered everything and clicked on \"create\", you will be redirected to the details page of that crowd heuristic. As you can see in Fig. 3, the state is still `initial` and there is no annotator or slice selected. You can now fill in the required information, so assign one annotator account to one static data slice. After that, you can generate a link that can be sent out to the annotator. \n<CaptionedImage src=\"crowd-labeling/crowd_labeling_3.gif\" title=\"Fig. 3: GIF of the user filling in the required information for the crowd heuristic shortly after creation.\"/>", "Before sending out any links, you should be sure that you selected the right labeling task, annotator, and data slice. As soon as one crowd label is assigned to this heuristic, these options will be locked and cannot be changed anymore.\n\nOnce you made sure everything is correct, you can just send the link out to the person that has access to the selected annotator account (e.g. send the link to `moritz.feuerpfeil@onetask.ai` in the example of Fig. 3).\n<Note>\nWant to revoke access to this heuristic but already sent out the link? Selecting the lock icon below the link will \"lock\" the annotator out of this heuristic, which means they won't be able to annotate or view the data anymore. This way you can revoke access to the data without deleting the heuristic.\n</Note>\nAs you can see in Fig. 4, as soon as the off-screen annotator set the first label, the settings get locked. The progress bar indicated the progress of the annotator on the selected slice. Statics are collected and live updated without refreshing the page. Read more on [evaluating heuristics](/refinery/evaluating-heuristics) if you are curious about the meaning of the statistics.\n<CaptionedImage src=\"crowd-labeling/crowd_labeling_4.gif\" title=\"Fig. 4: GIF of the crowd heuristic details page while the annotator is labeling off-screen. Live updates show the engineer how far the annotator progressed and with what accuracy compared to the manual reference labels.\"/>", "Currently, there is no delete button on the crowd heuristic details page. That means the only way to delete this heuristic is by selecting it on the heuristic overview page and clicking on \"actions\" -> \"delete selected\".", "This is the official documentation of refinery - the data scientist's [open-source](https://github.com/code-kern-ai/refinery) tool of choice to scale, assess and maintain natural language data.\n\n<CaptionedImage\n  src=\"/refinery/screenshot-refinery.png\"\n  title=\"Fig. 1: Screenshot of refinery's data management browser.\"\n/>", "As you can tell from this documentation - refinery is our flagship product. It is incredibly powerful, and can help you build strong NLP.\n\n<Note>\n  In the documentation, you will see screenshots of refinery with the actual\n  Kern AI brand logo. Don't get confused by this, until recently, refinery's and\n  Kern AI's logo was the same. We just recently gave refinery a full standalone\n  logo :)\n</Note>", "The documentation is structured by features of refinery. You can start via the following [quickstart](/refinery/quickstart)!", "refinery comes with the following features.", "refinery comes with a [built-in editor](/refinery/manual-labeling) (incl. role-based access) supporting classifications, span-extraction and text generation. Further, you can export data to other annotation tools like Labelstudio.", "Use our modular [data management](/refinery/data-management) to find e.g. records with below 30% confidence and mismatching manual and automated labels, sorted by confidence. Assign that data either to an inhouse expert or a crowdlabeler.", "You love Hugging Face, GPT-X or cohere for their large language models? We do too. That is why we integrated them into refinery. You can use them for [embeddings](/refinery/embedding-integration) (and [neural search](/refinery/neural-search)), [active transfer learning](/refinery/heuristics#active-learners), or even to create the training data for finetuning these LLMs on your data.", "refinery is shipped with a Monaco editor, enabling you to [write heuristics](/refinery/heuristics) in plain Python. Use them for e.g. rules, API calls, regex, active transfer learning or zero-shot predictions", "In the [project dashboard](/refinery/monitoring), you can find distribution statistics and a confusion matrix showing you where your project needs improvement. Every analysis can be filtered down to atomic level.", "Yes, you read that right. Our flagship product is open-sourced under the Apache 2.0 license. You can find the code on [GitHub](https://www.github.com/code-kern-ai/refinery). We are also happy to accept contributions.", "Kern AI refinery is available as [open-source](https://github.com/code-kern-ai/refinery), so you can install it on your local machine. The open-source version can be used with only a single user and therefore doesn't contain any user management.\n\nIf you enjoy working with it, please consider giving us a [GitHub star \u2b50](https://github.com/code-kern-ai/refinery).\n\nYou can install the application by either cloning our repository or by installing it with pip. Due to the containerization of all services, one prerequisite is that you need to have **docker** installed on your local machine.", "As the open-source version is hosted on GitHub, just clone the repository.\n\n<CodeGroup>\n  git clone https://github.com/code-kern-ai/refinery.git\n</CodeGroup>\n\nAfter that, if you want to start the application you can use the start scripts we\ndeveloped for all operating systems.\n\n<CodeGroup>\n```bash {{ title: 'Mac / Linux' }}\ncd refinery\n./start\n```\n\n```bash {{ title: 'Windows' }}\ncd refinery\nstart.bat\n```\n\n</CodeGroup>\n\nTo stop refinery, you can use the stop scripts that shut down the containers in a controlled way.\n\n<CodeGroup>\n```bash {{ title: 'Mac / Linux' }}\n./stop\n```\n\n```bash {{ title: 'Windows' }}\nstop.bat\n```\n\n</CodeGroup>", "Refinery is also registered on [pypi](https://pypi.org/project/kern-refinery), which means you can install it using pip. Do not get confused with the displayed version number on pypi as we implemented a separate updating service that lets up update refinery without pushing a new version to pypi.\n\n<CodeGroup>\n```bash\npip install kern-refinery\n```\n</CodeGroup>\n\nStarting the service then pulls all the required containers and connects them, while stopping it will remove everything safely. Since it was installed with pip, these commands are operating system independent.\n<CodeGroup>\n```bash {{title: \"starting\"}}\nrefinery start\n```\n\n```bash {{title: \"stopping\"}}\nrefinery stop\n```\n</CodeGroup>", "After starting the app, you can visit `localhost:4455` access the UI. You now just have to register a user and after that will be redirected to the home screen.\n<Note>\nOnly register a single user!\nWhile in theory you could log out of the registered user and create a new one, we strongly advise against it as the open-source version is not built to handle multiple users and might break projects when doing so. If you need multiple users, please look at our managed version.\n</Note>\n\n<CaptionedImage src=\"installation/installation_1.png\" title=\"Fig. 1: Screenshot of the home screen directly after installation.\"/>\n\nFrom here, we really recommend trying the [quick start of this documentation](/refinery/quickstart), which gives you a good overview of refinery without taking too much of your time.", "Before you can label anything in refinery, you must first create a labeling task. Labeling tasks include information about the labeling target (full record or certain attribute), the type of the task, and the available labels. Each labeling task has a unique name that is used to identify it on other pages, e.g. in the labeling view and data browser.\n<CaptionedImage src=\"/refinery/labeling-tasks/1.png\" title=\"Fig. 1: Screenshot of the settings page, which displays the data schema, embeddings, labelings tasks, and project metadata. There are two registered labeling tasks: indicators and topic.\"/>", "The labeling task type defines the granularity of your labeling. Currently, we support these two options:\n\n- `Multiclass classification`: Gives you the option to assign the target record or attribute exactly one of the available labels. Good for downstream tasks like classification.\n- `Information extraction`: Gives you the option to assign any token of the selected attribute to exactly one label. Required the labeling task to be defined on an attribute, and not on the full record. Good for downstream tasks like named entity recognition, sentence segmentation, or part-of-speech tagging.\n<Note>\nWe're working on adding further types in the future, such as relationship labels or native multilabel classifications (which, at the moment, you'd need to build via a workaround of multiple multiclass classifications). Let us know if you need something that refinery does not offer at the moment!\n</Note>", "To add a labeling task, simply click on the \"Add labeling task\" button on the settings page.\n\nA modal will open up, which asks you for the attribute you want to label. This selection will determine the available task types for later. If you want to label for classification and don't want to differentiate between single attributes, go with the `full record` option. After also providing a unique name for the task, you can now create the labeling task.\n\n<CaptionedImage src=\"/refinery/labeling-tasks/2.png\" title=\"Fig. 2: Screenshot of the settings page where a user is adding a new labeling task called 'sentiment' on the target attribute 'headline'. This will give them the option for a classification or extraction task.\"/>", "Deleting a labeling task has far-reaching consequences as it is associated with labels, heuristics, and some filters in the data browser. If you delete this structure, the associations will be removed, too, which means that the labels and labeling efforts of that specific task will also be removed from the project.\n\nIf you are sure that you want to delete the labeling task, just click on the red trashcan icon on the very right of the labeling task on the settings page. There will be an explanatory modal that requires your confirmation (see Fig. 3).\n<CaptionedImage src=\"/refinery/labeling-tasks/3.png\" title=\"Fig. 3: Screenshot of the settings page where the user clicked on the red trashcan icon in order to delete a labeling task, which triggered the confirmation modal that you can see in this screenshot.\"/>", " ", "Labels can be created at any time during the project, both on the settings page and while labeling your records. This gives you a lot of flexibility if requirements change during the project.\n<Note>\nCreating information extraction labels automatically create lookup lists. For every new label created on an extraction task, refinery will automatically create a lookup list with the exact name of that label. If you label tokens with said label, it automatically gets added to the lookup list. This is just a convenience feature. You don't have to use the lookup lists, but they are often a great starting point for keyword [labeling functions](/refinery/heuristics#labeling-functions). Deletion of the label will NOT delete the associated lookup list, though.\n</Note>\n\n\nIn order to add labels on the settings page, you just have to press on the \"+\" icon, which will open a modal where you must enter a unique label name for that task. So you could use the same label names for different tasks (as can be seen in Fig. 2). Users oftentimes want to add more than just a single label, which is why the modal stays open even after adding the label (shortcut confirm with enter key). That way you can add multiple labels really fast and when you're done, just close the modal with the \"close\" button (see Fig. 4).\n\n<CaptionedImage src=\"/refinery/labeling-tasks/4.png\" title=\"Fig. 4: Screenshot of the settings page where the user clicked on the '+'-icon at the right side of a labeling task, which triggered this modal, where unique label names can be entered to create new labels.\"/>\nYou can also add labels while in the labeling suite (see Fig. 5) by typing the name of the new label into the search bar and pressing the \"+\" icon next to the search bar. This will add the new label to the available options, which you will have to manually select afterward in order to label your record.\n\n<CaptionedImage src=\"/refinery/labeling-tasks/5.gif\" title=\"Fig. 5: GIF of a user adding a new label to the labeling task 'topic' in the labeling suite.\"/>", "Sometimes, you might choose the wrong name for a label, or you just want to shorten it because it clutters your labeling view. To stay flexible throughout the project, you can rename labels from the settings page. In order to do that you have to click on the little color pipette icon on the left side of the label. A modal will appear that lets you customize your label with a color and a keyboard shortcut, but if you want to rename it, you have to click on the label itself at the very top of that modal (see Fig. 6).\n\n<CaptionedImage src=\"/refinery/labeling-tasks/6.gif\" title=\"Fig. 6: GIF of a user accessing the label renaming.\"/>\nWhen renaming the label, refinery is aware that this label might have been used in heuristics, lookup lists, or other parts of your project. This is why there is a mandatory check before you can actually rename the label. This will display all the parts in refinery where this label name appears. Please keep in mind that we provide a \"best guess\" for these changes. Since custom written Python code is very versatile some changes might not be what you intended.\n<CaptionedImage src=\"/refinery/labeling-tasks/7.png\" title=\"Fig. 7: Screenshot of the label renaming process after pressing the 'check rename' button. The displayed warnings remind you where the current label name is used.\"/>\n<Note>\nBe careful with renaming the lookup lists. Currently, if you have multiple labels with the same name in different tasks, it could occur that renaming a classification label will display a change warning for a lookup list with the same name that was created from an extraction task. So before renaming a lookup list, be sure that it is not linked to an existing information extraction label.\n</Note>", "Deleting a label will also delete all the manually labeled data associated with it, as the given label would have no reference to a label and labeling task anymore. The other labels and tasks will be unaffected.\n\nIn order to delete a label, just go to the settings page and click on the little trashcan icon right next to it (not the one for the labeling task!). As this will delete all the manual labels associated with this label, there will be a modal asking for confirmation.", " ", "You can customize your labels for more efficient labeling. If you want to change the color of your label, just click on the little pipette icon next to it on the settings page. That page also allows you to set a unique keyboard shortcut for that label. Just press the desired key, which will then be saved automatically. The chosen shortcuts will also be displayed on the settings and labeling suite.", "Even though `refinery` offers a free open-source version, there are tons of reasons to consider the managed version. Just to name a few:\n\n- multi-user labeling with inter-annotator agreement\n- extensive role system\n- crowd-labeling functionality\n- no setup hassle\n- priority / dedicated support\n- full platform (i.e. access to [gates](/gates) and [workflow](/workflow))\n\nFor pricing information, please visit the [pricing page](https://www.kern.ai/pricing).", "If you want to test out the managed version of refinery, we offer a 14-day free trial - no payment information required! Visit our [app plattform](https://app.kern.ai/) and sign up (see Fig. 1).\n\n<CaptionedImage\n  src=\"managed-version/1.png\"\n  title=\"Fig. 1: Screenshot of the platform login screen. Here you can sign up for your 14-day free trial.\"\n/>\n\nAfter signing up, you will see a screen saying that you've been added to the waitlist (see Fig. 2). This is because we will manually check signups and will notify you when your 14-day trial starts. This will usually not take very long and is much faster if you were in contact with us previously.\n\n<CaptionedImage\n  src=\"managed-version/2.png\"\n  title=\"Fig. 2: Screenshot of the platform welcome screen directly after signup.\"\n/>\n\nOnce you were granted access, you will see a dashboard overview of all available applications of Kern AI (see Fig. 3). Here, you can click on `refinery` in the application selection. If this is your first time, we recommend starting with the [quickstart](/refinery/quickstart).\n\n<CaptionedImage\n  src=\"managed-version/3.png\"\n  title=\"Fig. 3: Screenshot of the platform welcome screen once you've been granted access.\"\n/>\n\n<Note>\n  If you want to register multiple users to test out the multi-user support,\n  please contact us.\n</Note>", "The [managed version](/refinery/managed-version) of refinery comes with multi-user support extended by a comprehensive role system.", "The role system in refinery consists of three distinct roles:\n\n- **Engineers**:  Administer the project and mostly works on programmatic tasks such as labeling automation or filter settings. They have access to all features of the application, including the Python SDK.\n- **Domain experts**:  Working on manual reference labels, which are used as ground truth throughout the application (e.g. for active learners and evaluation). They have access to the labeling view only but can freely choose between static data slices to work on.\n- **Annotator**: Working on manual labels as if they were a heuristic. As their labels are collected as a heuristic, the engineers can exclude them during weak supervision if their quality is too low. They have access to a task-minimized labeling view only, where they can only work on the data they were assigned. Engineers can revoke their access to the labeling view.", "If you want to see what users are currently registered within your organization, click on the honeycomb icon in the top right, directly next to the notification center.\n<CaptionedImage src=\"/refinery/managing-roles/1.png\" title=\"Fig. 1: Screenshot of the user overview page.\"/>", "To invite new users to your organization, please contact your support manager. We will quickly invite the new users and set up the desired roles for you.", "Manual labeling is a crucial part of creating high-quality training data as the domain experts provide meaning and context, which the algorithm can then use to recognize patterns and make informed decisions. Because it is such an important part, we spent much time making this experience in refinery as comfortable as possible.\n\nThis page will cover the labeling capabilities of refinery in a single-user setting. If you are looking for [multi-user features](/refinery/multiuser-labeling) please visit the dedicated page on that. Everything on this page still applies to the general labeling workflow.\n\n<CaptionedImage\n  src=\"/refinery/manual-labeling/1.png\"\n  title=\"Fig. 1: Screenshot of the labeling suite. The user hovers over a manually given label for an extraction task, which shows the corresponding position in the 'headline' attribute.\"\n/>", "Manual labeling happens exclusively in so-called labeling sessions. A labeling session consists of not more than 1000 records and can be started from two locations: the data browser and the navigation bar. Labeling sessions are persisted in the database, but more on that later.", "If you start a labeling session from the navigation bar (label icon, 3rd option from the top in Fig. 1), you will always get the first 1000 records of your data in the default order. That order is determined during data upload and is not influenced by refinery, but rather by the database itself. Additionally, you will always start at the first position of those 1000 records.", "Starting a labeling session from the data browser is the most preferred scenario for manual labeling as it gives you full flexibility. To start a session, you first have to select the records that you want to label. This is because the labeling session will always consist of the records that are visible in the data browser at that time and will also have the same order as in the data browser. After you selected your filters or data slice, you can click on \"Continue with this record\" on the desired record (see Fig. 2).\n\n<CaptionedImage\n  src=\"/refinery/manual-labeling/2.png\"\n  title=\"Fig. 2: Screenshot of the data browser where a user selected the data slice that they want to label. The labeling session will start after clicking on 'continue with this record'.\"\n/>\nAfter clicking \"Continue with this record\", you will be redirected to the labeling\nsuite with the first 1000 records of the selection (see Fig. 3).\n\n<Note>\n  Need to label more than 1000 records of a single data slice? In that case you\n  could split up that slice according to an attribute like \"running_id\". Select\n  the data slice in the data browser, add the positional filter (e.g.\n  \"running_id\" > 1000) and start the session from there!\n</Note>", "For a frustration-free experience, it is important to understand when things are persisted and when they are lost. Every labeling session that was started is persisted in the database and can be accessed again at later times. Refinery persists the records of the labeling session, not the filter settings. Because of this design, you can even access labeling sessions where the corresponding data slice or filter settings have been deleted. Also because of this design, you cannot retrieve the filter settings that were used to create a labeling session. So if you did not save your filter settings of the data browser as a data slice, you will lose those settings once you start the labeling session. The best practice for this scenario is either saving it as a data slice or starting the labeling session in a new tab.\n\n<CaptionedImage\n  src=\"/refinery/manual-labeling/3.png\"\n  title=\"Fig. 3: Screenshot of the labeling suite, where a new labeling session was just started (indicated by the notification on the bottom left).\"\n/>\nBut even though labeling sessions are persisted in the database, there currently\nis no UI element that lets you select past labeling sessions. This is why Fig. 3\nalso contains the URL, which we are going to inspect now. The URL after the domain\nhas the following template: `/app/projects/$PROJECT_ID/labeling/$SESSION_ID?pos=$POS&type=$TYPE`.\nRelevant for us is the `SESSION_ID` because by altering this, you can access previously\npersisted labeling sessions. There might be a UI element in the future for this,\nbut at the moment, this is how you can still access old labeling sessions. If you\ndon't want to leave refinery, we suggest saving important sessions as [data slices](/refinery/data-management#from-filters-to-data-slices)\nor at least their IDs as [comments](/refinery/comments).\n\nEntering a valid labeling session ID that has not been persisted in the database creates a new one with the default records and default ordering.\n\n<Note>\n  If you want to jump far ahead to a certain position in the labeling session,\n  modify the \"pos\" attribute in the URL.\n</Note>", " ", "Ideally, you have everything set up already before starting your labeling session. But if you should come across a record in your labeling workflow that deserves a label that has not been registered yet, you can create that new label directly from the labeling suite (see Fig. 4). More on creating labels in [this section](/refinery/labeling-tasks#labels).\n\n<CaptionedImage\n  src=\"/refinery/manual-labeling/4.gif\"\n  title=\"Fig. 4: GIF of a user adding a new label to the 'topic' labeling task within the labeling suite. The procedure for adding an extraction label is the same.\"\n/>", "As there are two different types of labeling tasks, there are also two different types of labels you can assign. Every label must be assigned to a certain labeling task, which is why you need to have at least one classification or extraction labeling task. As soon as there is a task available, it will be displayed in the labeling suite. If it is a task defined on an attribute, it will be displayed next to that attribute (see \"indicators\" in Fig. 5). If it is a classification labeling task defined on the whole record, it will be displayed next to \"Full Record\" (see \"topic\" in Fig. 5).\n\n<CaptionedImage\n  src=\"/refinery/manual-labeling/5.png\"\n  title=\"Fig. 5: Screenshot of the labeling suite with two registered labeling tasks.\"\n/>", "Classification labels are displayed without any user interaction. If you want to assign a label, just click on the colorful badge or press the keyboard shortcut that you assigned on the settings page. This adds a new entry to the label table below the record.\n\nEvery classification task can only carry a single label for one record as refinery currently supports multiclass classification labeling, but not multi-label classification.", "Extraction labels are a little different because they do not label whole records or attributes, but label only spans (or tokens if you will) within a textual attribute. In order to select the span that you want to label, you simply drag across it with your mouse. You don't have to select all the right characters of a single token, refinery expands that selection to the whole token automatically. You can see how the tokenizer split up your data by hovering over the text. This will also display the automatically detected entities as tooltips if it is supported by the spaCy model that you selected at project creation (see [spaCy documentation](https://spacy.io/usage/linguistic-features#named-entities)).\n\n<CaptionedImage\n  src=\"/refinery/manual-labeling/6.gif\"\n  title=\"Fig. 6: GIF of a user labeling for an extraction task. Note how the user did not make a perfect selection, but refinery matched selected the entire tokens.\"\n/>\nAfter you made your selection, a modal will appear that displays the available label\noptions for that task. Just select the label you want to add to the highlighted span.\nIf there are multiple extraction tasks on the same attribute, they are displayed\nbeneath each other (see Fig. 6).\n\nEvery span can have any amount of labels.", "If you already ran weak supervision on your data, there will be additional badges for the weak supervision predictions (if not [hidden by the user](/refinery/manual-labeling#customizing-the-labeling-view)). The weak supervision classification label can be recognized by the confidence and hexagonal-ring icon on the badge, a weak supervision extraction label will be displayed as a non-deletable dotted line below the span.\n\nTo accept the weakly supervised label for a classification task, just click on the badge. A weakly supervised label for an extraction task must be set manually without assistance.\n\n<CaptionedImage\n  src=\"/refinery/manual-labeling/7.gif\"\n  title=\"Fig. 7: GIF of a user adopting the weakly supervised labels as manual ones.\"\n/>", "Deleting individual manually set labels is as easy as clicking the delete icon. This can be the delete icon right next to the label in the record, or the trashcan icon in the label overview table, both work identically. There will be no confirmation prompt.\n\nIf you want to bulk delete a label and all the assigned manual labels with it, you have to delete that label from the task itself (see [here](/refinery/manual-labeling#customizing-the-labeling-view)).", "Some people want to see all the available information on their screen, some like to stay minimalistic. Because of personal preferences and also possible biasing, we allow for a high level of customization within the labeling suite.", "There are several different types of labels one can have in refinery. And it does not always make sense to display all of them in the labeling suite as this could bias the annotating person towards a label that was given by a heuristic or weak supervision, which would undermine the validation and improvement of them. To put the user in full control, refinery allows for very granular display options.\n\n<CaptionedImage\n  src=\"/refinery/manual-labeling/8.png\"\n  title=\"Fig. 8: Screenshot of the labeling suite where all label types are displayed. You can see weak supervision labels (diamond icon), predictions of active learners (lightning bolt icon), and manual labels on this particular screenshot.\"\n/>\nDirectly above the record display are buttons that control the visible labels and\npredictions. The position of the circle is representative of the displayed type,\ne.g. top left circle stands for manual label. To display - say - the weak supervision\nlabels, click on \"Weak Supervision\", which will add these labels to the view. The\nbuttons that are visible by default do not reflect the state, they are only for setting\nthe state (see Fig. 9).{' '}\n\n<CaptionedImage\n  src=\"/refinery/manual-labeling/9.gif\"\n  title=\"Fig. 9: GIF of the user selecting the visible label types. Note how the selection is additive, e.g. selecting 'weak supervision' does not reset the 'manual' selection, but adds on top of it to display both.\"\n/>\nIf you want to inspect the state, you have to open the drawer menu (see Fig. 10).\nHere you can also set the visible types for each labeling task and label individually.\n\n<CaptionedImage\n  src=\"/refinery/manual-labeling/10.gif\"\n  title=\"Fig. 10: GIF of the user opening the drawer of the label type display settings to individually activate all label types for the label 'sport' of the task 'topic'.\"\n/>", "The labeling suite settings can be accessed on the bottom right corner of the screen indicated by a gear icon. Here, you can further customize your labeling suite UI. The most important settings include hiding the label overview table, displaying heuristic confidence, and expanding the classification label display.\n\nAll the options are explained in the UI and will not be further discussed here for the time being.", " ", "For easier identification of labels and labeling tasks, refinery implements a highlighting feature. Whenever you hover over a label, refinery will highlight the corresponding entry in the label overview table below the record.\n\nYou can also hover over individual rows of the table to see them highlighted in the record. If you want to highlight just a single row, make sure to hover over the magnifying glass icon on the very right. You can also highlight whole groups of labels by hovering over their shared information (e.g. the same labeler, task, or type).\n\nCustomize your hovering color in the labeling suite settings in the bottom right corner for a personal touch!", "The labeling workflow comes integrated with a lightweight record IDE. Here you can take a look at a specific record from a programmatic point of view, which helps in prototyping labeling functions or testing outside APIs with requests. For the full documentation, please visit the dedicated [record IDE page](/refinery/record-ide).", "<Note>\nCurrently only available for classification models. We're testing this feature on classification models for now. If it is widely requested, we'll add a callback for extraction models.\n</Note>\nModel callbacks allow you to feed predictions (and their confidences) of trained models into refinery, where they can be used for filtering and data slice creation. Consider this another great tool for quality control as it allows you to create validation slices where the production model disagrees with the manual or weakly supervised label.\n\nFor documentation on the exact usage with examples, please look at the relevant section on the [GitHub readme](https://github.com/code-kern-ai/refinery-python-sdk#callbacks) of the refinery SDK. There are many different wrappers already available, e.g. HuggingFace, PyTorch, or Sklearn.\n\nOnce you uploaded your predictions to refinery, you can access the model callbacks on the heuristics page (see Fig. 1). \n<CaptionedImage src=\"model-callbacks/1.gif\" title=\"Fig. 1: GIF of the user accessing the model callbacks through the heuristic overview page.\"></CaptionedImage>\n\nFig. 2 shows how the user can use callbacks in combination with manual labels to find incorrect model predictions with a high confidence. This filter setting also goes both ways, as it simultaneously is a way to detect labeling mistakes.\n\n<CaptionedImage src=\"model-callbacks/2.png\" title=\"Fig. 2: Screenshot of the user filtering for hard wrong predictions of the model using the model callback values.\"/>", "The overview page (also sometimes referred to as \"monitoring page\") was designed to keep track of what's happening in your project. You can inspect the labeling progress, amount of heuristics, label distribution, and much more. The best thing about it: you can display all those statistics and graphs on pre-defined slices of your data and for singular labeling tasks.", "Before inspecting statistics and charts about your project, always make sure to select the right target, labeling task, and data slice at the top of the page.\n\nThat being said, the easiest way to get a glance at how far the project progressed is by inspecting the aggregated statistics at the top of the page. Here, you can see the amount of manually labeled data, weakly supervised data, heuristics, and finally (if you're on the [managed version](/refinery/managed-version)) the inter-annotator agreement. You can hover over all of these to get more details.\n\nThe following subsections will cover the available visualizations.", "This grouped bar chart (see Fig. 1) shows you the distribution of labels given, grouped by the label names. The different colors indicate whether the label has been set manually or by weak supervision. If you hover over the bars, you will get more details.\n\nThis chart is very useful to spot under- or over-represented classes, which might indicate the necessity of splitting up or merging two labels in order to get a relatively balanced training dataset. Also, the distribution of the manually labeled and weakly supervised data should roughly match as you otherwise would over- or under-represent classes through the strong scaling of weak supervision. If they do not match roughly, consider adding more heuristics to the classes that are struggling to keep up and consider temporarily removing heuristics of the overrepresented class.\n<CaptionedImage src=\"/refinery/monitoring/1.png\" title=\"Fig. 1: Screenshot of the monitoring page that shows the label distribution chart.\"/>", "The confidence distribution chart displays the percentiles of weakly supervised data plotted against the weak supervision confidence score. This chart is important to inspect in combination with the total amount of weak supervision labels, as you are usually most interested in the data that was labeled with high confidence. The chart is to be read as follows: in Fig. 2 you see that the 40th percentile is plotted against a ~90% confidence score. That means that 40% of your weakly supervised data is roughly between 0 - 90% confidence.\n\n<CaptionedImage src=\"/refinery/monitoring/2.png\" title=\"Fig. 2: Screenshot of the monitoring page that shows the confidence distribution chart.\"/>", "The confusion matrix is your key visualization in understanding weak supervision quality. It plots the weakly supervised labels against the manual reference labels, which lets you see where weak supervision succeeds and fails at a quick glance. This is especially powerful in combination with the data browser filtering as you can then just select the labels with the highest disagreement and manually confirm if these were in fact mistakes of weak supervision or labeling mistakes. Use this chart to iteratively refine your heuristics and weak supervision.\n<CaptionedImage src=\"/refinery/monitoring/3.png\" title=\"Fig. 3: Screenshot of the monitoring page that shows the confusion matrix.\"/>", "If you are on the managed version of refinery, you automatically get multi-user support. As there are now potentially many different labelers, there also might be different opinions among their labeling. With the inter-annotator agreement, you can see how much user agree or disagree at a single glance. This can help you bring people together who seem to have a different understanding of the labeling task at hand. For example in Fig. 4, the user with the red avatar should really talk to the other labelers.\n<CaptionedImage src=\"/refinery/monitoring/4.png\" title=\"Fig. 4: Screenshot of the monitoring page that shows the inter-annotator agreement chart.\"/>", "You can also reduce the record set that is being analyzed on the overview page by selecting a **static** data slice in the top right dropdown. This way, all graphs and statistics will be filtered, giving you deeper insights into your potential weak spots.\n<CaptionedImage src=\"/refinery/monitoring/5.png\" title=\"Fig. 5: Screenshot of the monitoring page where the user filtered the statistics and visualizations to only consider the static data slice called 'Germany-related'.\"/>\nTo learn more about data slices, read the page on [data management](/refinery/data-management#saving-filters).", "The [managed version](/refinery/managed-version) of refinery offers multi-user support, which is generally recommended for the highest label quality as a majority voting among multiple domain experts usually yield better results than a single individual labeling everything. With multiple users also come more requirements, like quantifying general disagreement and quick ways to solve conflicts.\n<CaptionedImage src=\"/refinery/multi-user-labeling/1.png\" title=\"Fig. 1: Screenshot of the labeling suite where the displayed record was labeled by two different users, which is indicated by the two avatars next to the 'record IDE' button on the top left. Selecting an avatar lets you inspect the labels that they assigned to this record.\"/>", "When you and a colleague have opposing opinions on labeling a record, which label should be considered by refinery for accuracy calculation, monitoring, or training purposes? At the moment, none of those labels will be considered for these tasks because the system cannot decide which is the correct one. To address this issue, refinery introduces the concept of gold labels (sometimes called gold star labels).\n\nThe gold label is a special label for refinery because it will always be prioritized over regular labels. In the single-user application, there is no need for this distinction, but it is necessary for resolving conflicts. Currently, there is no automated way of calculating gold star labels (e.g. majority vote) as we found that it is often times necessary to discuss the labeling conflicts with your domain experts first in order to clear up any confusion about the task.\n<CaptionedImage src=\"/refinery/multi-user-labeling/2.gif\" title=\"Fig. 2: GIF of a user looking at the label of their colleague and deciding it must have been a mistake. In order to solve the conflict, they select their own label as the gold star label. When accessing the gold label view, a modal explains why you should treat it with care.\"/>\nThere are two ways of assigning a gold label:\n\n- If at least two different people assigned conflicting labels, an empty star icon will appear next to the labeling task where the conflict occurs. In order to select a gold label, first make sure you selected the right view (e.g. your own view as your colleague made a mistake) by clicking on the corresponding avatar and then clicking on the star icon. After that, the star icon will be filled.\n- As soon as there is a gold label, you can access the gold label view to directly edit the gold labels. This is especially useful for conflicts in extraction tasks, as oftentimes a combination of labels from different users is necessary to correctly label the record.", "Currently, if you want a colleague to have a look at your data, there is no way of sharing an exact record. You could inform your colleague that they need to look for the 30th record in a certain data slice, but that is not really intuitive. That is why we recommend sharing labeling sessions. They work differently depending on the role that the user \n\nAs explained in the separate labeling session section, you can share your sessions by sharing the URL with a colleague. The colleague will then start at the exact position that you shared with the same records in the same order. Important is that this person also has access to your project, so they must be in the same organization in refinery. Also, this just works", "Oftentimes the engineer is not the person doing most of the labeling work. Instead, the engineer can create static data slices that the experts can then label themselves. They will have a much more restricted view of refinery as they cannot access anything beyond the labeling suite. For more information on the different roles, please look at the [managing roles](/refinery/managing-roles) page of this documentation.\n<CaptionedImage src=\"/refinery/multi-user-labeling/3.png\" title=\"Fig. 3: Screenshot of the labeling suite view of a user with the 'Expert' role. They can freely select available static data slices to work on.\"/>\nLabels by experts are treated as regular labels, just like the ones of an engineer.\n\nAn engineer can temporarily view a data slice as an expert to confirm that this is what they intended. In order to do that, navigate to the data browser and click on the information icon next to a static data slice. There you will see a link that lets you take the temporary view of an expert (see Fig. 4). This is also the link that you can share directly with your expert colleagues.\n\n<CaptionedImage src=\"/refinery/multi-user-labeling/4.gif\" title=\"Fig. 4: GIF of an engineer accessing the temporary expert view of a static data slice.\"/>", "In cases where you want to assign labeling work to people who might lack domain expertise, you can use the [crowd-labeling](/refinery/heuristics#crowd-labeling) heuristic of refinery. The annotators will have the same labeling suite view as the experts with the exception that they cannot freely select data slices. Instead, they can only work on the heuristic that was manually assigned to them.\n<CaptionedImage src=\"/refinery/multi-user-labeling/5.png\" title=\"Fig. 5: Screenshot of the labeling suite view of an annotator. They can just select the crowd heuristic and do not even see what data slice is behind that.\"/>", "Neural search refers to the concept of search in an embedding space that is created by neural networks. Instead of searching for co-occurring n-grams to retrieve similar records, neural search uses the context-rich embeddings generated by large pre-trained language models and a distance metric in that space to define similarity between records. This similarity can be used for both use cases: for finding similar data, but also for detecting outliers. \n\n One pre-requisite to using neural search is that you already [added embeddings](/refinery/embedding-integration) to the project.", "Every record in the data browser has the option to \"find similar records\" which will calculate the cosine similarity using the selected embeddings. After selecting the embedding, the data browser then displays records with descending similarity starting from the record that you requested the similarity search for (as it is the most similar to itself). \n\nWhen using similarity search you cannot filter for anything else. Doing so will replace the similarity search. Similarity search cannot be saved as a data slice.", "We make use of vector distance comparison to find records that are - given some vector space - most likely outliers. To use this feature, we need at least one labeled record, as we compare pools of unlabeled and labeled data for this outlier detection (even though to really make sense you should have labeled more).\n\nThe outlier detection will create a data slice containing 100 records that are (on average) the most different (or _least similar_) from the already labeled data. The similarity will be measured by cosine distance in the embedding space. This data slice will be ordered by ascending similarity.\n\nThis feature is accessible at the very bottom of the data browser filter sidebar. Just click on the \"Find outliers\" button there.\n\n<CaptionedImage src=\"/refinery/neural-search/1.gif\" title=\"Fig. 1: GIF of a user creating a static outlier data slice in the data browser.\"/>\nThe results of the outlier detection are heavily dependent on the vector space. Especially when used as a filter criterium for the [monitoring](/refinery/monitoring) page, you can quickly find weak spots or/and obstacles in your data, e.g. detect faulty records or completely unrelated languages.", "To start a new project, you have two options: either you create a project from scratch (\"new project\" button) or you already have a project that was exported as a snapshot at some point that you want to continue working on (\"import snapshot\" button). \n<CaptionedImage src=\"/refinery/project-creation/project_creation_1.png\" title=\"Fig. 1: Screenshot of the refinery home page where the user already has one project (from our [quickstart](/refinery/quickstart)) and wants to add another one.\"/>\nAs the second option does not need any further information (you just drag and drop the project export), we will further go into detail on how to get started with a brand new project.", "A project cannot be created without data, which is why you should have the desired dataset ready to go. If you are not sure if your data fits the requirements of refinery, please look at the data requirements section of this page.\n<CaptionedImage src=\"/refinery/project-creation/project_creation_2.png\" title=\"Fig. 2: Screenshot of the project creation page.\"/>\nFor now, we assume you have your data ready and want to create a project with it. The workflow is really straightforward as you just enter a title and description, select the right tokenizer for your text data, and upload a file.\n\nThe tokenizer serves two purposes in refinery:\n\n- defining atomic information units in your texts, which is required for span labeling\n- precomputing valuable metadata for each token, which is then available for labeling functions and other tasks\n\nIf you need a `spaCy` tokenizer that is not available in the selection, please add it in your [configuration page](/refinery/configuration-page). After adding it, create a new project and it should be available.\n\nCurrently, refinery supports data upload via file. In the near future, refinery will also enable uploads via API (and Python SDK) and database integrations. \n\n<CaptionedImage src=\"/refinery/project-creation/project_creation_3.gif\" title=\"Fig. 3: GIF of the user creating a project.\"/>\nAs you can see in Fig. 3, after filling in every required information and uploading your data, pressing \"proceed\" will redirect you to the settings page of the freshly created project. The tokenization process is displayed below the record schema, which in this case completed instantly because our `test_upload.json` just consists of four records. Make sure to select the primary key of your data directly after the project creation.\n\n<CodeGroup>\n    ```json {{title: \"test_upload.json\"}}\n[\n    {\n        \"headline\": \"Mike Tyson set to retire after loss\",\n        \"running_id\": 0\n    },\n    {\n        \"headline\": \"Iraqi vote remains in doubt\",\n        \"running_id\": 1\n    },\n    {\n        \"headline\": \"Conservatives Ponder Way Out of Wilderness\",\n        \"running_id\": 2\n    },\n    {\n        \"headline\": \"Final report blames instrument failure for Adam Air Flight 574 disaster\",\n        \"running_id\": 3\n    }\n]\n```\n```bash {{title: \" \"}}\n```\n</CodeGroup>\n\nIf you are wondering what to do next, why not visit our [quickstart](/refinery/quickstart)? It guides you through the application and you will learn a ton about refinery in the meantime.", "By default, the open-source version of refinery imposes some changeable restrictions on the data you upload, namely the maximum amount of records, the maximum amount of attributes, and finally the maximum amount of characters for a record. Visit the documentation of the [configuration page](/refinery/configuration-page) to see if your data fulfills these requirements and, if necessary, change them to your needs.\n\nIf you are within the set limits, we can now proceed to look at the data formats. You can upload CSVs, JSONs, or Excel spreadsheets. In the backend, the uploaded file is processed with `pandas`, so you can specify import options for your files just as you would for reading [data frames](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html). Just input them into the text box below the upload field (see Fig. 2). Each option must be in its own line and you do not have to use quote characters to symbolize strings. If you're not sure which parameters you can specify, have a look at the documentation pages ([JSON](https://pandas.pydata.org/docs/reference/api/pandas.read_json.html), [CSV](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html), [spreadsheets](https://pandas.pydata.org/docs/reference/api/pandas.read_excel.html)).\n\nRefinery offers support for the following datatypes: `category`, `text`, `integer`, `float`, and `boolean`. Currently, there is no support for date- or time-related data types.\n\n<Note>\nOnly `text` attributes are available for tokenization and embedding creation.\n</Note>", "If you already have (partially) labeled data, you can add these labels to the uploaded file but must follow a certain structure. Refinery groups labels in so-called [labeling tasks](/refinery/labeling-tasks), which need a name and a target. The target can either be a single attribute or the whole record.\n\nTo add the labels to your data, you have to add an attribute with the following schema: `$attribute__$name`, where the values of this attribute are the labels themselves. If you don't have attribute-level labels, you can just leave the `$attribute` empty, which will be then recognized as a label for the full record.", "<CodeGroup>\n```json {{title:\"attribute_labeled_data.json\"}}\n[\n    {\n        \"headline\": \"Mike Tyson set to retire after loss\",\n      \t\"text\": \"He lost and went >SEE YA, IM OUT<, lol\",\n        \"running_id\": 0,\n        \"headline__sentiment\": \"neutral\",\n      \t\"text__style_of_speech\": \"informal\"\n    },\n    {\n        \"headline\": \"Iraqi vote remains in doubt\",\n      \t\"text\": null,\n        \"running_id\": 1,\n      \t\"headline__sentiment\": null,\n        \"text__style_of_speech\": null\n    }\n]\n```\n```bash {{title: \" \"}}\n```\n</CodeGroup>\n<CaptionedImage src=\"/refinery/project-creation/project_creation_4.png\" title=\"Fig. 4: Screenshot of the settings page after uploading `attribute_labeled_data.json`.\"/>", "<CodeGroup>\n```json {{title:\"full_record_labeled_data.json\"}}\n[\n    {\n        \"headline\": \"Mike Tyson set to retire after loss\",\n        \"running_id\": 0,\n        \"__sentiment\": \"neutral sentiment\",\n        \"__relevant\": \"not relevant\"\n    },\n    {\n        \"headline\": \"Iraqi vote remains in doubt\",\n        \"running_id\": 1,\n      \t\"__sentiment\": null,\n          \"__relevant\": \"relevant\"\n    }\n]\n```\n```bash {{title: \" \"}}\n```\n</CodeGroup>\n<CaptionedImage src=\"/refinery/project-creation/project_creation_5.png\" title=\"Fig. 5: Screenshot of the settings page after uploading `full_record_labeled_data.json`.\"/>", "You can upload more data to the project at all times by visiting the settings page. The new data must have the same signature (amount, names, and types of attributes) and cannot violate the primary key restrictions that must be set on your data. If you have not set a primary key yet, you should do that because it is the only way to avoid duplicates!\n<CodeGroup>\n```json {{title:\"more_data.json\"}}\n[\n    {\n        \"headline\": \"What Should You Bring To Your Office's White Elephant Gift Exchange\",\n        \"running_id\": 5,\n        \"__sentiment\": \"neutral sentiment\",\n        \"__relevant\": \"not relevant\",\n        \"__clickbait\": \"clickbait\"\n    },\n    {\n        \"headline\": \"Kim Sets a Skating Record and Wins Her First World Title\",\n        \"running_id\": 4,\n      \t\"__sentiment\": \"neutral sentiment\",\n        \"__relevant\": \"relevant\",\n        \"__clickbait\": \"not clickbait\"\n    }\n]\n```\n```bash {{title: \" \"}}\n```\n</CodeGroup>\nYou can, however, add new labeling tasks and labels by uploading new data, which is illustrated in Fig. 6, where the project of Fig. 5 is extended with the new `more_data.json` data.\n\n<CaptionedImage src=\"/refinery/project-creation/project_creation_6.gif\" title=\"Fig. 6: GIF of the user adding `more_data.json` to the project from Fig. 5. Note how the user first makes sure to set the primary key. Before the upload, the project had two labeling tasks. The upload added the new labeling task `clickbait`.\"/>", "There is a lot of value in having a running ID attribute in refinery as it allows you to filter your data with simple integers. If refinery detects that you do not have an integer attribute in your data, refinery automatically creates a `running_id` attribute. In case you do not want to see that attribute, you can always hide it using the [attribute visibility](/refinery/attribute-visibility) settings.", "refinery is awesome and full of complex features, which makes it such a great tool. That also comes with the downside that completely new users might feel overwhelmed at first. This walkthrough aims at eliminating this initial feeling! We will create a project with easy-to-understand data which we will use to explain most of the important features of refinery. In the end, you will have a labeled dataset ready for prototyping models and a good understanding of refinery itself!\n\nIf you have any questions, would like a personal onboarding, or just want to leave feedback, we are highly available on our [community Discord server](https://discord.gg/qf4rGCEphW). Drop by and we will be happy to help!\n\nFor this walkthrough you will need:\n\n- (if you use the open-source, self-hosted version:) an already [installed](/refinery/installation) version of refinery\n- between 15-30 minutes of time", "After following the installation guide, you should know the refinery home page. Here we will now create a sample project that comes with two things: a pre-loaded dataset and two labeling tasks. But more on that in a second, first we have to create the project. Click on \"sample projects\" and select \"initial\" directly below the \"clickbait\" sample use case (see 1). We want to load up the initial version because we want to simulate starting from scratch.\n\n<CaptionedImage\n  src=\"/refinery/quickstart/quickstart_1.png\"\n  title=\"Fig. 1: Screenshot of the refinery home screen after installation. The user is selecting the clickbait sample project in its 'initial' state.\"\n/>\n\nAfter creation, you will be redirected to the first page of your project - the overview page (see Fig. 2). Here you can [monitor your labeling progress](/refinery/monitoring) and follow aggregated stats. As we haven't done anything in the project yet, this will be rather empty. Let's go ahead and inspect the data next!\n\n<CaptionedImage\n  src=\"/refinery/quickstart/quickstart_2.png\"\n  title=\"Fig. 2: Screenshot of the overview page. It is empty because we did not label a single thing yet.\"\n/>", "In order to see what data is available to us, we will head over to the second page of refinery: the [data browser](/refinery/data-management). For that, we select the second icon in the navigation bar on the left (hover over the icons to inspect their names). As you can see in Fig. 3, the data is rather simple: a `running_id` and a single `headline` for each record.\n\n<CaptionedImage\n  src=\"/refinery/quickstart/quickstart_3.png\"\n  title=\"Fig. 3: Screenshot of the data browser. The left side consists of filtering capabilities and the larger right side is an infinite scrollable list of your records.\"\n/>\n\nThe data seems to be properly imported without any corruptions or artifacts, which should always be the first sanity check when creating a new project. There are two things we notice about this page, though. The first thing is that it says \"no [embedding](/refinery/embedding-creation)\" under every record. The second thing is that there are two filter settings on the left side that refer to [labeling tasks](/refinery/labeling-tasks). Let us look at both of those things in more detail on the settings page (gear icon in the navigation bar).\n\n**Optional challenge:** find out if there is any other headline in the data mentioning Mike Tyson.", "The project settings page has three important components: the data schema, embeddings, and labeling tasks. Let us first understand the data schema.\n\n<CaptionedImage\n  src=\"/refinery/quickstart/quickstart_4.png\"\n  title=\"Fig. 4: Screenshot of the settings page of the newly created clickbait project.\"\n/>", "The data schema (Fig. 4) depicts all the attributes of your data with their respective data type. This is the place where you can upload more records that fulfill this data schema, [download your records and project](/refinery/data-export), or even [add new attributes](/refinery/adding-attributes) by writing them as python functions (really powerful and fun). We won't do any of this for the time being, so we will continue with the embeddings.", "If you are familiar with NLP, you know that embeddings are a vector representation of your text data carrying semantic information. This embedding is a requirement for many features of refinery, like [neural search](/refinery/neural-search), [active learners](/refinery/active-learners), [zero-shot classification](/refinery/zero-shot-classifiers).\n\nIn order to use those features, we will now create an attribute-level embedding of our headline attribute with the `distilbert-base-uncased` model from huggingface.\n\n<CaptionedImage\n  src=\"/refinery/quickstart/quickstart_5.gif\"\n  title=\"Fig. 5: GIF of the user adding an attribute-level embedding to the project.\"\n/>\n\nThe notifications in the bottom left corner signal us that the process of encoding has finished. Refinery reduces the dimensionality of your embeddings automatically to save disk space (without significant performance losses). For more information about embeddings in refinery, read the dedicated section about it.\n\nFor now, all we have to know is that we added semantic vector representations of our data to our project, which unlocks many features! Let us continue with the settings page and look at the existing labeling tasks.", "When you want to label something, you have to first define it as a labeling task. For example, we want to label if headlines are clickbait or not, which means we need a classification labeling task. Luckily, this project already came with two labeling tasks: one for classification, and one for information extraction (see Fig. 4 or Fig. 5). A classification task can only carry one label per record or attribute (for us it is either \"yes\" or \"no\") while an information extraction task labels individual parts of the text, just like a text marker would. That is why the extraction task is defined on an attribute (target is \"headline\").\n\nYou might think that we don't need the extraction task because we just want to classify clickbait articles and not do any Named Entity Recognition on this data, but extraction tasks come with a neat feature that we will make use of later. They are directly linked to lookup lists, which are collections of terms that are automatically generated when we label.\n\nOne thing bothers us though, and that is the inexpressiveness of those label names \"yes\" and \"no\". Let us change those labels to better reflect what they represent. Refinery is built to handle changing requirements during the project, which is why renaming labels is possible at all times.\n\n<CaptionedImage\n  src=\"/refinery/quickstart/quickstart_6.gif\"\n  title=\"Fig. 6: GIF of the user renaming the labels for the classification task. The label 'yes' becomes 'clickbait' and the label 'no' becomes 'not clickbait'.\"\n/>\n\nWe are now ready to finally label some data! Let us visit the labeling suite, which is accessible by clicking the little label icon in the navigation bar on the left.", "Manual labeling is a crucial part of gathering high-quality training data, which is why we won't skip it for this quickstart. Refinery handles manual labeling in so-called \"labeling sessions\", which are just a sequence of 1000 records. When just clicking on the labeling icon in the navigation bar, we are presented with the first 1000 records of our project in the default order. Read more on labeling sessions on the [manual labeling](/refinery/manual-labeling) page of this documentation.\n\n<CaptionedImage\n  src=\"/refinery/quickstart/quickstart_7.png\"\n  title=\"Fig. 7: Screenshot of the labeling suite. For this quickstart, we will only focus on the attributes of the presented record and the labeling options. All the other surrounding features are explained in the manual labeling documentation.\"\n/>\n\nThe labeling suite is generally really powerful because it is highly customizable to your needs. You are in complete control of the amount of information displayed on the screen, which by default looks like Fig. 7. Assigning classification labels is as easy as clicking on them (or pressing their keyboard shortcut). Extraction labels are defined on spans, which you first have to select with your mouse.\n\nFor this labeling part, we will first label a record if it is clickbait or not, and then label (if possible) the part of the headline that influenced that decision. By doing that, we are collecting more information about our decision-making, which we can use for some labeling automation in later steps. One available label for span labeling is called `country_related`, which we will use to indicate what parts of non-clickbait headlines refer to political events or entities, which is almost never clickbait but real news. Label about 20-30 examples (see Fig. 8) so we have some data to work with later!\n\n<CaptionedImage\n  src=\"/refinery/quickstart/quickstart_8.gif\"\n  title=\"Fig. 8: GIF of the user labeling some records. First, they label the classification, then they look for expressions in the headline that helped in that decision.\"\n/>\n\n**Optional challenge:** start a labeling session with random unlabeled data from the data browser.", "Now that we have labeled a bunch of data, we already suspect certain patterns to indicate whether a headline is clickbait or not. For example, many clickbait headlines started with a number or addressed the reader directly (saying \"you must\" or \"why you should\"). If these patterns really do indicate the class of a headline, we would like to write these patterns down as [labeling functions](/refinery/labeling-functions), helping us in accelerating the labeling process. But before we can do that, we have to confirm our suspicions, which we will do in the data browser, so we can head over there now by clicking on its icon in the navigation bar!\n\nThe first pattern we want to look at is if most of the records starting with a number are actually clickbait. For that, we open the filtering options for \"result order\" and order our records by the headline (ascending, see Fig. 9). This works just like the ordering in your file explorer, which has all special characters coming first, then numbers, then letters.\n\n<CaptionedImage\n  src=\"/refinery/quickstart/quickstart_9.png\"\n  title=\"Fig. 9: Screenshot of the data browser where the user selected to order the records by the headline attribute in ascending order. The user already scrolled past the headlines starting with special characters.\"\n/>\n\nWhen scrolling through these records, we confirm our suspicion that _most_ of these are clickbait headlines. These patterns will not be used as a single source of truth and more like a heuristic, which is why they don't have to be perfect!\n\nThe next pattern we want you to validate yourself! For that, clear the existing filter by clicking on the \"clear filter\" button in the top right area of the data browser. After that, find a way to filter your data according to the headline attribute containing words that are addressing a person directly, e.g. \"you\" or \"your\". If you need some help, the [data management documentation](/refinery/data-management) should contain the information you want! In case you don't want to or cannot solve this task, just continue with the quickstart.\n\n**Optional challenge:** label a few records that are similar to the one where the `running_id` is six (hint: use refinery's neural search feature).", "You probably gave refinery a shot because you heard it saves you a lot of time by leveraging modern labeling techniques that go way beyond manual labeling. We're just about to do exactly that! Refinery allows you to leverage heuristics, which are imperfect label predictors that are, in the end, aggregated to a single label with a process called weak supervision.\n\nThe best heuristic to explicitly transfer domain knowledge to the system is a labeling function. A labeling function lets you make a prediction for your data with Python code. Let's write a labeling function that captures the patterns we discovered in the earlier section.\n\n<CaptionedImage\n  src=\"/refinery/quickstart/quickstart_10.png\"\n  title=\"Fig. 10: Screenshot of the heuristics overview page. Currently, there is no heuristic registered, which is why there are a lot of explanations displayed. They will disappear once you create a heuristic, so make sure to read them before If you want to.\"\n/>\n\nHead over to the heuristics overview page by clicking on the light bulb icon in the navigation bar (see Fig. 10). After that, select \"new heuristic\" and \"labeling function\" at the top of the screen. Make sure to select the right labeling task (`clickbait`).\n\n<CaptionedImage\n  src=\"/refinery/quickstart/quickstart_11.gif\"\n  title=\"Fig. 11: GIF of the user creating a new labeling function.\"\n/>\n\nAfter creation (see Fig. 11), you will be redirected to the labeling function details page, where we will configure and code the function. If you're wondering what this \"search in bricks\" means, bricks is an open-source NLP content library maintained by us, which has many common NLP tasks pre-coded for you to choose from. If you're curious, take a look at [bricks](https://bricks.kern.ai/home) and the [bricks integration page](/refinery/bricks-integration) of this documentation.\n\nFor this quickstart, we will start from scratch. Let us write a function that splits the headline into a list of words and looks if the first word is a number. If it is, we are going to return the label `clickbait`. If it is not, we cannot really make a prediction and don't return anything.\n\nCopy the following code and paste it into the code editor of the labeling function details page:\n\n<CodeGroup>\n\n```python\ndef starts_with_digit(record): headline = record[\"headline\"].text\n  headline_as_list = headline.split(\" \") if(headline_as_list[0].isnumeric()):\n  return \"clickbait\"\n```\n\n </CodeGroup>\n\n\n<CaptionedImage\n  src=\"/refinery/quickstart/quickstart_12.png\"\n  title=\"Fig. 12: Screenshot of the labeling function details page where the user finished writing their labeling function in the code editor.\"\n/>\n\nAfter we have written the function, we are going to validate that it works as intended without any bugs by running it on a few examples. For that, we select the headline attribute in the selection on the bottom right and press the \"Run on 10\" button.\n\n<CaptionedImage\n  src=\"/refinery/quickstart/quickstart_13.gif\"\n  title=\"Fig. 13: GIF of the user testing the function on 10 randomly sampled records twice.\"\n/>\n\nWe then scroll down to observe the results. As you can see in Fig. 13, the first \"Run on 10\" yielded no prediction as no record started with a number. We just reran it to select another 10 random records, which returned some true positives. As we gained confidence in this function, we will now run it on all records! For that, just press the \"Run\" button, which is right next to the \"Run on 10\" button that we just used.\n\n**Optional**: copy the code of `starts_with_digit` over to the [record IDE](/refinery/record-ide) in order to test the functionality on single records.", "Remember that we labeled individual spans for the information extraction task? This effort will now come in handy as we will integrate the automatically created lookup lists into labeling functions that will return a label if words on that list appear in the headline.\n\nFor that, we will first inspect the lookup lists. Let us go back to the heuristic overview page and click on the \"lookup lists\" button on the top right (see Fig. 14).\n\n<CaptionedImage\n  src=\"/refinery/quickstart/quickstart_14.gif\"\n  title=\"Fig. 14: GIF of the user inspecting the lookup lists that were automatically filled by labeling an extraction task.\"\n/>\n\nThese lists are not overly crowded at the moment, but we can still use them in labeling functions. The good part: with every label we assign, we will keep filling these lists with useful terms to determine the class of a record.\n\nLet us write two new functions, one for each class. Fig. 15 shows how to do it for the `no clickbait`class, you can do the other one by yourself! You can take the same code and just exchange the imported list to `clickbait_expression`, the name of the function to `lookup_clickbait`, and the returned label to `clickbait`. Make sure to select the correct labeling task `clickbait`!\n\n<CodeGroup>\n\n    ```python\n    from knowledge import country_related\n\n    def lookup_not_clickbait(record):\n    for term in country_related:\n    if(term.lower() in record[\"headline\"].text.lower()):\n    return \"no clickbait\"\n\n    ```\n\n</CodeGroup>\n\n<CaptionedImage\n  src=\"/refinery/quickstart/quickstart_15.gif\"\n  title=\"Fig. 15: GIF of the user writing a labeling function that utilizes the lookup list created during labeling.\"\n/>", "Remember that we added `distilbert-base-uncased` embeddings to our project at the beginning? Well, these embeddings contain a lot of information from powerful pre-trained transformer models. It would be a shame if we would not use this to our advantage!\n\nThat is why we will now create a new heuristic: the active learner. The active learner is a simple machine learning model that learns on your already manually labeled reference data and then makes predictions on the rest of the data. The model can be very simple because most of the heavy lifting has already been done by the powerful transformer that encoded our texts.\n\nTo add an active learner (see Fig. 16), we just create it similar to the labeling function. Fill in the required information during creation and all the code will be pre-generated for you!\n\n<CaptionedImage\n  src=\"/refinery/quickstart/quickstart_16.gif\"\n  title=\"Fig. 16: GIF of the user adding an active learner.\"\n/>\n\nThe code that is automatically generated should suffice in 80% of cases. While the structure of this code must stay the same, you could exchange modules of it, e.g. change the classifier from a LogisticRegression to a DecisionTree, exchange the embedding to a better one, or tune the `min_confidence` parameter. Look at the [active learner documentation](/refinery/active-learners) for more information on this!\n\nWe will now reap what we've sown and combine the information of all heuristics into a single label.", "As we previously mentioned, weak supervision combines all the information of the selected heuristics into a single label. The quality of this label will usually be pretty good, but still is bound to the first law of machine learning: garbage in, garbage out.\n\nThat is why we will first select all heuristics and run them (see Fig. 17) to update them. With the latest run, we can then estimate their quality.\n\n<CaptionedImage\n  src=\"/refinery/quickstart/quickstart_17.gif\"\n  title=\"Fig. 17: GIF of the user running all heuristics to bring them up to date.\"\n/>\n\nAfter they all finished their execution (see Fig. 18), we can now see the estimated precision, which is one of the most important metrics for weak supervision. At the moment, they are all very over-confident, which usually is bad and means that we have not labeled enough data or not enough diverse data. We could validate the heuristics by labeling the data that they made predictions on, but for the sake of this quickstart, we will just move on to the weak supervision.\n\n**Note:** your precision estimations could look a little different depending on what records you have labeled.\n\n<CaptionedImage\n  src=\"/refinery/quickstart/quickstart_18.png\"\n  title=\"Fig. 18: Screenshot of the heuristics overview page. All heuristics are finished and have an estimated precision of 100%, which indicates over-confidence.\"\n/>\n\nTo weakly supervise our data, we just select all heuristics that we want to incorporate and click on the colorful button at the top (see Fig. 19).\n\n<CaptionedImage\n  src=\"/refinery/quickstart/quickstart_19.gif\"\n  title=\"Fig. 19: GIF of the user running weak supervision. After it finished, they inspect information about the last run.\"\n/>", "After we applied weak supervision, we can now revisit the overview page. Click on the pie chart icon at the very top of the navbar. Here you'll see how much data was labeled both manually and through weak supervision (see Fig. 20). You can also see the distributions, which are important as they should roughly match between manual and weakly supervised.\n\n<CaptionedImage\n  src=\"/refinery/quickstart/quickstart_20.png\"\n  title=\"Fig. 20: Screenshot of the overview page after weak supervision.\"\n/>\n\nWe now have labeled half of the dataset with relatively little effort, which is really awesome. If you scroll down on the overview page, you will see both a confidence distribution chart and a confusion matrix. The confusion matrix is really important to understand the quality of your weakly supervised labels.\n\nGenerally, this process only becomes better and better over time as you add more granular heuristics covering a wide spectrum of domain knowledge. Also, it becomes more accurate the more manually labeled reference data you have in your project. You could already export the first version of this data and prototype a classifier to test against some validation data you already have. But instead of doing that, we will show you one last feature that helps in validating your heuristics, which also helps the weak supervision process to figure out which source of information is more correct than others.", "There are many useful filters in the data browser. You could for example specifically filter for records where different heuristics disagree among themselves (see Fig. 21). This is especially powerful to manually correct the over-confidence of certain heuristics.\n\n<CaptionedImage\n  src=\"/refinery/quickstart/quickstart_21.png\"\n  title=\"Fig. 21: Screenshot of the data browser where the user filtered for conflicting heuristics on the clickbait labeling task.\"\n/>\n\nTo validate all heuristics at the same time, we could search for records which the heuristics made a prediction for, but no manual label was yet assigned. To make this validation easily accessible for future sessions, we want to save this filter setting. All of this is done in Fig. 22, where we first filter the data and then save it to a so-called [data slice](/refinery/data-management). We chose the dynamic data slice so that every time we use that slice, the filter settings are re-applied to the data. That means after we labeled a little bit of that slice, the labeled data does not fulfill the filter criteria anymore and won't be included the next time we select it - awesome!\n\n<CaptionedImage\n  src=\"/refinery/quickstart/quickstart_22.gif\"\n  title=\"Fig. 22: GIF of the user filtering for unlabeled data where at least one heuristic made a prediction. They then save it as a dynamic data slice for reusability.\"\n/>\n\nIf you want to reuse the data slice, just click on it in the data browser and the filter will automatically be reapplied (see Fig. 23).\n\n<CaptionedImage\n  src=\"/refinery/quickstart/quickstart_23.png\"\n  title=\"Fig. 23: Screenshot of the data browser where the top left shows the data slice we previously saved.\"\n/>\n\n**Optional challenge:** create a data slice that consists of records that have a weakly supervised label and no manual label. Sort that data slice by ascending weak supervision confidence.", "You almost reached the end! There is just one final thing to do: export the data. Developing training data with refinery is an iterative process that allows for fast prototyping and constant _refinement_. There are a lot of ways of exporting your data, for this quickstart we will choose the simplest one: download it as a JSON. For that, go to the settings page and press on \"download records\" beneath the data schema. You can leave everything on default, press \"prepare download\" and then download the zip file. Always make sure to press \"prepare download\" before actually downloading because otherwise, you would download outdated data (we save the last export).\n\n<CaptionedImage\n  src=\"/refinery/quickstart/quickstart_24.gif\"\n  title=\"Fig. 24: GIF of the user exporting the labeled data.\"\n/>", "That concludes this quickstart. **Thank you** so much for staying with us until this point, we hope you enjoyed it :)\n\nIf you have any feedback, questions, or just nice words, you can reach us on our [community Discord](https://discord.gg/qf4rGCEphW)! We would love to have you chatting there. Or, if you wish to show us some github love, you can do that on [refinery](https://github.com/code-kern-ai/refinery).\n\nYou have now seen large parts of the application and got a feeling for the important workflows. From here, you could grab some data of yours and create a new project with it! Learning by doing is after all the most efficient way to master refinery.\n\nBut if you want to stay a little longer in this demo project, here are a few suggestions to further explore refinery:\n\n- Wonder why we renamed the label names at the beginning? Add a [zero-shot classifier](/refinery/zero-shot-classifiers) to the heuristics!\n- Improve your weak supervision by [validating existing heuristics](/refinery/evaluating-heuristics).\n- You're not satisfied with the existing attributes? Use Python code to [generate new ones](/refinery/adding-attributes)!\n- Already got a model for spam detection and want to feed its predictions into refinery? Try [model callbacks](/refinery/model-callbacks).\n- Want to collaborate with your colleagues? Check our [managed version](/refinery/managed-version) with [multi-user labeling](/refinery/multi-user-labeling) capabilities!", "The record IDE lets you take a look at specific records from a programmatic point of view. From here you can prototype labeling functions, explore the spaCy Doc structure of your text attributes, or test a new regex.\n<CaptionedImage src=\"/refinery/record-ide/1.png\" title=\"Fig. 1: Screenshot of the labeling view. You can access the record IDE from here by clicking on the second button from the left on the top.\"/>", "As can be seen in Fig. 2, the record IDE is split in half and consists of a code editor on the left and an output shell on the right. There is one pre-set variable called `record`, which is a single dictionary where the keys are the names of the attributes (same format as the input to a [labeling function](/refinery/heuristics#labeling-functions)). You can use this variable to interact with the record that is currently viewed.\n<CaptionedImage src=\"/refinery/record-ide/2.png\" title=\"Fig. 2: Screenshot of the record IDE. The user printed the record variable and some information about its attributes. The code input is on the left, and the printed output is on the right.\"/>\nThe right-most button brings you to the installed libraries that you can import and use in this IDE. They are identical to the ones for the labeling functions to allow for labeling function prototyping.\n<CaptionedImage src=\"/refinery/record-ide/3.png\" title=\"Fig. 3: Screenshot of the record IDE where a user wrote invalid code. The error can be inspected in the output shell.\"/>\nTo run the code, you can either press SHIFT + ENTER on your keyboard or press the \"run\" button directly above the output shell.\n\nThere is also built-in navigation (\"back\" & \"next\" buttons above the code editor), so you can try your code on all records of the current labeling session. After navigating to the next or previous record, your code will automatically be executed. Generally, we recommend to be careful if you experiment with rate-limited APIs in the record IDE.", " ", "Just like the [bricks integration](/refinery/bricks-integration) for labeling functions, you can also use pre-written bricks modules in the record IDE for prototyping and testing. Click on the \"search in bricks\" button above the output shell and follow the integration. The generated code will be automatically executed once after the integration finished.", "Code is only auto-saved within a single record IDE session. That means if you leave the page, the code will reset to the default `print(record)`. But if you prototyped a labeling function that you want to save for later, you can do so by pressing the \"save\" button above the code editor.  This will save the code to your local browser storage, which only holds a single entry.\n\nCurrently, there is no feedback on whether or not the code was saved, but don't worry about it as this is a local operation in the browser without any communication to services that could fail.\n\n<CaptionedImage src=\"/refinery/record-ide/4.gif\" title=\"Fig. 4: GIF of the user demonstrating the bricks integration and then saving the generated code to access it again in a later session.\"/>", "We update refinery regularly and generally recommend staying on the latest version as this unlocks new features or enhances existing ones. There are also a ton of bugfixes with every release, so make sure to look out for our changelogs! If you run into any issues during updating, make sure to let us know on [Discord](https://discord.com/invite/qf4rGCEphW) or in the [GitHub issues](https://github.com/code-kern-ai/refinery/issues).\n\nIf you want to update, simply click on the version number at the bottom of the left sidebar. A modal will open up, showing you all services used in this version, including the current version on your machine as well as the remotely available version.\n\n<CaptionedImage src=\"/refinery/updating-refinery/updating_refinery_1.png\" title=\"Fig. 1: Screenshot of the modal that appears after clicking the version number in the bottom left. It shows all available services and their versions.\"/>\nIf your version is not the latest, click on \"how to update\" to view a step-by-step guide to updating refinery (see Fig. 2). Simply follow the short instructions there, and if you should run into any problems, let us know through discord or create an issue on the refinery repository!\n<CaptionedImage src=\"/refinery/updating-refinery/updating_refinery_2.png\" title=\"Fig. 2: Screenshot of the modal that appears when you click on the 'how to update' button from Fig. 1. It describes the updating procedures for all available operating systems.\"/>", "You've set up some really great heuristics, and are now wondering how well they work together? Easy - we can now apply weak supervision to create automated and denoised labels.", "Weak supervision is a technique that allows you to combine multiple heuristics to create a single, automated label. The radicality lies in the fact that you don't need to label the training data entirely by yourself, but only a part of it.\nThen, you can use the heuristics you've already set up to create labels for your training data. This way, you can create a training set that is much larger than the one you would have created by labeling the data yourself.\nThe key idea is that labeling functions don't need to be perfectly accurate, they can be noisy. You can combine multiple heuristics that you have already set up to create a single, denoised label.", "Let us go through an example to see how weak supervision works in refinery. We will use the same example as in the [heuristics](/refinery/heuristics) section. You can start off by navigating to the heuristics page.\nMake sure you have some heuristics created beforehand. If you don't, you can create some by following the [heuristics](/refinery/heuristics) section.\n\n<CaptionedImage\n  src=\"/refinery/weak-supervision/weak-supervision-1.png\"\n  title=\"Fig. 1: This page brings all the heuristics together to be collectively used in weak supervision.\"\n/>\n\nNow select the heuristics you want to combine, simply by clicking the checkbox. Once you've done that, press the \"Weak supervision\" button. This will start the weak supervision process.\n\n<CaptionedImage\n  src=\"/refinery/weak-supervision/weak-supervision-2.png\"\n  title=\"Fig. 2: Weak supervision best works if you have a collection of lableing functions or active learners.\"\n/>\n\nThat's it! It usually also takes up only a few seconds to compute the labels. After the process is finished, you will get a notification prompt on the bottom left of the page notifying whether or not it is completed or ran into some error.\n\n<CaptionedImage\n  src=\"/refinery/weak-supervision/weak-supervision-3.png\"\n  title=\"Fig. 3: The weak supervision is successfully run. If it is not successful, a message in red will appear.\"\n/>\n\nThe button on the right side of the \"Weak supervision\" button summarizes the last weak supervision run you've executed. If you want some details, you can open a modal by pressing that button.\n\n<CaptionedImage\n  src=\"/refinery/weak-supervision/weak-supervision-4.png\"\n  title=\"Fig. 4: The button on the right of `Weak supervision` summarizes the last weak supervision run.\"\n/>\n\nNow that your remaining data has been labeled, you can have an overview of the performance of the weak supervision by taking a look into the label and confidence distributions and the confusion matrix.  \nLet's jump directly into the [monitoring](/refinery/monitoring) to see the quality of our weakly supervised labels.", "Classifiers are the modules that summarise a text into a specific category for example classify a set of news records into \u201cpolitics\u201d and \u201cbusiness\u201d. {{ className: 'lead' }}\n\nApart from that, the modules concerning enrichments like language detection are also put into classifiers. The available classifiers can be viewed as a collection as shown below.\n\n<CaptionedImage\n  src=\"/bricks/classifiers-overview.png\"\n  title=\"Fig. 1: Overview of classifiers.\"\n/>\n\nAs you can see, the classifiers are sub-categorised according to their execution types - \u201cPython function\u201d, \u201cPremium\u201d and \u201cActive learner\u201d. More information is provided on this in the later part of this documentation (see [execution types](/bricks#execution-types)).\n\nLet us go through an example of the \u201clanguage detection\u201d classifier. You can navigate to the module page by clicking `Language detection` .\n\n<CaptionedImage\n  src=\"/bricks/language-detection-1.png\"\n  title=\"Fig. 2: Language detection brick.\"\n/>\n\nThe module page shows the code snippet meant to detect languages. This code snippet field is non-editable, you can only copy-paste the code into refinery.\n\nOn the right side of the page, you can find an input field with a default input text. This input can be modified by you if you want to test the module over multiple texts.\n\nWhen you click on `Run sample` , the output is generated in the form of a dictionary.\n\n<CaptionedImage\n  src=\"/bricks/language-detection-2.png\"\n  title=\"Fig. 3: Execution of the language detection brick.\"\n/>\n\nAnd done! You can see, in this case, what language your text is in. In refinery, you can create a new attribute or a new heuristic and import the classifier module using the bricks integration feature which is an upgrade of simply copy-pasting the code snippet in refinery manually.\n\n<Note>\n  The `GitHub` button in front of the module name redirects you to the issue\n  page where, in case of malfunction, you can re-open the issue, or open a new\n  issue if you come up with a new idea for a classifier.\n</Note>", "These are the modules that extract some specific information from a given text. {{ className: 'lead' }}\n\nThe extractors bring along a lot of use-cases with them, like extracting addresses from a given text, or extracting date and time values from a text.\n\n<CaptionedImage\n  src=\"/bricks/extractors-overview.png\"\n  title=\"Fig. 1: Overview of extractor bricks.\"\n/>\n\nLet us go through an example of using the hashtag extraction which can be accessed by clicking the Hashtag extraction .\n\n<CaptionedImage\n  src=\"/bricks/hashtag-extraction-1.png\"\n  title=\"Fig. 2: A brick to extract hashtags from texts.\"\n/>\n\nOn the right side of the page you can see the input example which requires the text input and, for this case, also the SpaCy tokenizer due to the fact that most of the modules use SpaCy for extraction. The input fields may vary from module to module. In refinery, we support only the `en_core_web_sm` (English) and `de_core_news_sm` (German) tokenizers for now. In case you require additional tokenizer, reach out to us at info@kern.ai.\n\nWhen you click on `Run sample` the output is generated in the form of a dictionary as shown below\n\n<CaptionedImage\n  src=\"/bricks/hashtag-extraction-2.png\"\n  title=\"Fig. 3: Executed brick identified hashtags in some text.\"\n/>\nAs you can see, the endpoint response returns a dictionary which contains the position\nof both - the hashtag and the word attached to it, and assigns a label (\u201dhashtag\u201d\nin this case) to it. In refinery, you can choose your own label when importing the\nextractor module.\n\n<Note>\n  The `GitHub` button in front of the module name redirects you to the issue\n  page where, in case of malfunction, you can re-open the issue, or open a new\n  issue if you come up with a new idea for a extractor.\n</Note>", "As the name suggests, the generators are meant to create new textual content based on some given text. {{ className: 'lead' }}\n\nThis can include, for example, language translation where you have some data - say - in English and you wish to generate the same text in - say - German.\n\n<CaptionedImage\n  src=\"/bricks/generators-overview.png\"\n  title=\"Fig. 1: Overview of bricks generators.\"\n/>\n\nLet us go through an example using the language translation which can be accessed by clicking Language translator .\n\n<CaptionedImage\n  src=\"/bricks/language-translation-1.png\"\n  title=\"Fig. 2: A brick to translate between languages.\"\n/>\n\nOn the right side you can see the input example which, in this case, requires the text input, the language in which your provided text is, and the language in which you want to generate the text. Likewise, different modules can have different fields based on their functionality.\n\nOn clicking the `Run sample` button, the output here is generated in the form of a dictionary.\n\n<CaptionedImage\n  src=\"/bricks/language-translation-2.png\"\n  title=\"Fig. 3: Translates from German to English.\"\n/>\nThe endpoint response returns a dictionary which, in this case, shows the translated\ntext into English.\n\n<Note>\n  The `GitHub` button in front of the module name redirects you to the issue\n  page where, in case of malfunction, you can re-open the issue, or open a new\n  issue if you come up with a new idea for a generator.\n</Note>", "bricks is our content library that provides off-the-shelf natural language modules like language translation, sentiment classification, keyword extraction, etc. which you can use in your project directly or in refinery to enrich your text data. {{ className: 'lead' }}\n\n<CaptionedImage\n  src=\"bricks/bricks-hero.png\"\n  title=\"Fig. 1: Screenshot of bricks.\"\n/>\n\nThe library is available at [bricks.kern.ai](http://bricks.kern.ai) and its purpose\nis to make it easier for you to build better products using its distinguished modular\nautomations stacked together into one collection. This is not a library in the sense\nthat you can pip install it, but have to copy-paste the code from the online platform.\n\nIt consists of modules that can be categorised into three sections - classifiers, extractors and generators that are accessible from the left panel below the \u201chome\u201d icon.", "bricks - i.e. classifiers, extractors, or generators - are sub-categorised into \u201cPython function\u201d, \u201cActive learner\u201d, or \u201cPremium\u201d.\n\nThe \u201cPython function\u201d badge is associated with the modules which can be executed in the bricks environment as well as in refinery without requiring any external changes in the code.\n\n\u201cActive learner\u201d badge is associated with modules that are active transfer learning models. They can only be used in your project in refinery and not in bricks directly, since they require a dataset with some manual labelling.\n\n\u201cPremium\u201d badge is associated with the modules that require an external API key/token. You can contact us at info@kern.ai if you require an API key. Due to confidentiality, we have only chosen the examples for this documentation which are marked with \u201cPython function\u201d.\n\nIn bricks, you can also see at the end of the modules page that you can add your own module.\n\n<CaptionedImage\n  src=\"bricks/execution-types.png\"\n  title=\"Fig. 2: Execution types in bricks.\"\n/>\n\nIf you come up with an idea for either a classifier, extractor, or generator, you can open a new issue on GitHub and if we like the idea, we will implement the module for you, or you can implement the module by yourself by following the steps shown in [this](https://www.youtube.com/watch?v=S0cPZ5fqsNo&t=39s&ab_channel=KernAI) video.\n\nIn order to access the bricks modules into refinery, the proper way is to use the bricks in refinery [integration](/bricks/refinery-integration).", "bricks comes with the following features.", "Every brick is open-source and modular, such that you can customize the code itself immediately. Start with a great baseline, and adapt it to your needs.", "Every brick is designed to be [integrated into refinery](/bricks/refinery-integration). You can search within refinery for a brick, and then add it to your project within a few clicks.", "We aim to add new bricks every week. If you have a great idea for a brick, please let us know. We are happy to help you implement it.", "Just as refinery, bricks is [open-source](https://www.github.com/code-kern-ai/bricks). You can find the code on GitHub. We are also happy to accept contributions.", "Great, you're now set up with an API client and have made your first request to the API. Here are a few links that might be handy as you venture further into the Protocol API:\n\n- [Find out how to classify text](/bricks/classifiers)\n- [See which span labelers exist](/bricks/extractors)\n- [Learn more about generators](/bricks/generators)", "In refinery, sometimes you might want to run a code snippet for which there already is a brick module. {{ className: 'lead' }}\n\nTo make it easier for you to know what brick modules are available to be imported in refinery, there is a bricks integration feature which helps you to import a brick module for your use case. The integrator asks you to assign a value to the user defined variables or change the default values of the variables as needed. Once provided, the code is ready to be run in refinery.\n\nAn important thing to remember is that the bricks integration is highly dependent on the refinery version you might be using. If you are using an older version of refinery, chances are that you cannot access some of the brick modules which are only compatible with the newer version. The refinery version is visible on the bottom left panel of refinery.\n\nThis section provides a demonstration of using bricks in a refinery project using the bricks integration. For this documentation, we will use the \u201clanguage detection\u201d brick module.\n\nFor starters, we can create a new attribute on the settings page by clicking the `Add new attribute` button.\n\n<CaptionedImage\n  src=\"/bricks/new-attribute.png\"\n  title=\"Fig. 1: Adding a new attribute to refinery.\"\n/>\n\nOnce you click Add attribute you will be redirected to this newly created attribute page, where the editor would look like this:\n\n<CaptionedImage\n  src=\"/bricks/ac-details.png\"\n  title=\"Fig. 2: Details of the attribute calculation.\"\n/>\n\nAs you can see, there are two options: Search in bricks and Start from scratch . The first option is the gateway to the bricks integration feature which opens a prompt where you can select any of the available bricks for your project, as shown below.\n\n<CaptionedImage\n  src=\"/bricks/wizard-1.png\"\n  title=\"Fig. 3: Pick a brick from the setup wizard.\"\n/>\n\nOnce you have selected the module to import, you can go through the overview of the module which contains information and a link that redirects you to that module on bricks. If you want to know what a specific brick does, or need an example, you can navigate to Input example which has a default input that is also imported from bricks, and you can modify it to perform test runs.\n\n<CaptionedImage\n  src=\"/bricks/wizard-2.png\"\n  title=\"Fig. 4: See an input example.\"\n/>\n\nClicking the `Request Example` button generates the output that you expect to see for that default/user-defined input. Likewise, the `Reset to default` resets the prompt to its initial state.\n\nThe last part is the `Integration` feature which consists of fields that take user-defined values.\n\n<CaptionedImage\n  src=\"/bricks/wizard-3.png\"\n  title=\"Fig. 5: Choose the attributes for the input.\"\n/>\n\nOnce you have the variables set, you can look through the final code and changes.\n\n<CaptionedImage\n  src=\"/bricks/wizard-4.png\"\n  title=\"Fig. 6: Final review of your brick setup.\"\n/>\n\nLastly, you can click on Finish up and the code from bricks will be imported to the editor on the attribute page, as shown\n\n<CaptionedImage\n  src=\"/bricks/ac-details-2.png\"\n  title=\"Fig. 7: The integrated brick in your refinery module.\"\n/>\n\nAnd you are set! To test it out, you can run the code on 10 samples by clicking the button below the code editor and the output shall be generated in no time!\n\n<CaptionedImage\n  src=\"/bricks/results.png\"\n  title=\"Fig. 8: First results of the language detection module.\"\n/>\n\nBricks integration makes it more convenient for you to simply access the module without any hassles of finding the module specifically on the bricks app and worry if the module is compatible with your current refinery version or not. Apart from that, the above example was for attribute calculation, for which only classifiers or generators are to be used. In a similar fashion, you can use the bricks integration while creating a new heuristic - be it a labelling function (classifier/extractor) or an active learner.", "Deploying a model in gates is easy - we don't even need a _quickstart_. You will see how it works here. {{ className: 'lead' }}", "Before we can start deploying a model in gates, please make sure that you have a [refinery](/refinery) project up and running which contains a setup for [weak supervision](/refinery/weak-supervision).\n\n<CaptionedImage\n  src=\"/gates/gates-ready.png\"\n  title=\"Fig. 1: The Gates integration section says that the project is ready to use - if it says otherwise, you can just hit the _Update_ button nearby, which recalculates active learners and embedders.\"\n/>\n\nAlso, please make sure that you've [created an access token](/gates/token).", "As you head over to gates, you will see that your [refinery](/refinery) project is automatically synched with gates.\n\n<CaptionedImage\n  src=\"/gates/gates-sync.png\"\n  title=\"Fig. 2: Your gates projects are already synched from refinery. No need to create a project here manually.\"\n/>\n\nSimply click on the `Clickbait` title, and you will be redirected to your project.\n\n<CaptionedImage\n  src=\"/gates/gates-initial.png\"\n  title=\"Fig. 3: In here, you can now see the initial screen - i.e. a model that isn't deployed yet..\"\n/>\n\nIn here, you have a little overview. The left editor shows you a sample Python script to integrate the model once it is deployed - of course, you will have to exchange the token with your personal access token, and you need to enter other data than always the example record :)\n\nOn the rightern side, you see a playground, where you can validate example inputs. Let's first deploy the model, and then see the output of the playground. To deploy a model, simply hit _Open config_.\n\n<CaptionedImage\n  src=\"/gates/gates-setup.png\"\n  title=\"Fig. 4: All automated heuristics implemented in your refinery project are available in gates. Simply select the ones you'd like to choose for your gates API.\"\n/>\n\nAs you hit _Start_, the model will be deployed. For this, a dedicated container will be created, loading your embedders, active learners and other resources for the prediction - so there is a little overhead time. Usually, this takes between 10 seconds and 1 minute.\n\nOnce the state changes from `Pending` to `Running`, you can use the playground on the rightern side:\n\n<CaptionedImage\n  src=\"/gates/gates-running.png\"\n  title=\"Fig. 5: Your running gates project, including the playground on the righern side.\"\n/>\n\nTo stop or restart the model, simply use the _Open config_ bar anytime.", "Once you have deployed your model, you can use the gates API anywhere. Currently, you can use the API to:\n\n- classify records\n- make span predictions on texts\n- create new attributes\n\nIn general, you can do in gates on one record what you can do in refinery on many records. We're also aiming to integrate neural search to gates as soon as possible. To integrate gates in Python, copy the following script and exchange the project id, example records and token with your data. Generally, gates will expect exactly the same record schema as refinery.\n\n<CodeGroup tag=\"POST\" label=\"/commercial/v1/predict/project/0c2cb3af/prediction\">\n\n```python\nimport requests\n\nurl = \"http://localhost:4455/commercial/v1/predict/project/0c2cb3af\"\n\nheaders = {\n    \"accept\": \"application/json\",\n    \"content-type\": \"application/json\",\n    # make sure to replace <YOUR API KEY> with your own API key\n    \"Authorization\": \"0c2cb3af <YOUR_API_KEY>\"\n}", "example_data = {\n  \"headline\": \"Mike Tyson set to retire after loss\",\n  \"running_id\": 0\n}\n\nresponse = requests.post(url, headers=headers, json=example_data)\n\nprint(response.text)\n```\n\n</CodeGroup>\n\nThe output for the above example could look as follows, and we'll break down the structure below.\n\n```json {{ title: 'Response' }}\n{\n  \"record\": {\n    \"headline\": \"Mike Tyson set to retire after loss\",\n    \"running_id\": 0,\n    \"language\": \"en\"\n  },\n  \"results\": {\n    \"clickbait\": {\n      \"prediction\": \"no\",\n      \"confidence\": 0.9999367396527589,\n      \"heuristics\": [\n        {\n          \"name\": \"lkp_clickbait_expression\",\n          \"prediction\": null,\n          \"confidence\": null\n        },\n        {\n          \"name\": \"starts_with_digit\",\n          \"prediction\": null,\n          \"confidence\": null\n        },\n        {\n          \"name\": \"DistilbertLR\",\n          \"prediction\": \"no\",\n          \"confidence\": 0.9347412269488006\n        },\n        {\n          \"name\": \"DistilbertTree\",\n          \"prediction\": \"no\",\n          \"confidence\": 1\n        },\n        {\n          \"name\": \"lkp_country_related\",\n          \"prediction\": null,\n          \"confidence\": null\n        }\n      ]\n    },\n    \"named entities\": {\n      \"predictions\": [\n        {\n          \"confidence\": 0.9820137900379085,\n          \"prediction\": \"person\",\n          \"token_begin\": 0,\n          \"token_end\": 2,\n          \"char_begin\": 0,\n          \"char_end\": 10\n        }\n      ],\n      \"heuristics\": [\n        {\n          \"name\": \"person_extraction\",\n          \"predictions\": [\n            {\n              \"confidence\": 1,\n              \"prediction\": \"person\",\n              \"token_begin\": 0,\n              \"token_end\": 2,\n              \"char_begin\": 0,\n              \"char_end\": 10\n            }\n          ]\n        }\n      ]\n    }\n  }\n}\n```\n\n`record` is the data you input into the API call, enriched with the [attribute modifications](/refinery/adding-attributes) you configured in refinery. You can use gates to do just that, i.e. it is not necessary to have a classification or extraction prediction. If you want to do so, however, you will find them in the `results` part of the response.\n\nFor a classification, you can look into `results`, and then into your respective task (e.g. `clickbait`). On the task-level, it will contain a `prediction` - which is the final prediction for this task - and the `confidence`. If you want to break down the results, you can see which `heuristics` voted. To learn more about this process, check out our [weak supervision](/refinery/weak-supervision) page.\n\nFor an extraction, this looks similar. However, the tasks contain `predictions` items; you can have multiple extracted spans per text, which is why this is a list. Each item consists of the `prediction` and `confidence`, such as the `token_begin` and `token_end` (spaCy token indications), and the `char_begin` and `char_end` result (i.e. indices of characters per text). Again, you can also find the heuristic-level predictions.", "Now is a really good time to look into how you can put your gates API into action via [workflow](/workflow).\n\n- [See the gates AI node in workflow](/workflow/workflows#the-node-palette)\n- [Find an example use case to put gates into action](/use-cases/email-channels)", "You've set up a NLP project in [refinery](/refinery) and now would like to use it as a realtime API? Say no more. This is gates. {{ className: 'lead' }}\n\nWith gates, you get a lightweight add-on to refinery, allowing you to deploy a weakly supervised model within 3 clicks.\n\n<CaptionedImage\n  src=\"gates/gates-running.png\"\n  title=\"Fig. 1: Screenshot of a deployed model in gates.\"\n/>", "gates comes with the following features.", "You already did the work in refinery. Now gates is as simple as it gets. Select the automations, hit deploy, and your model is live.", "Integrate your model into any existing infrastructure. Use the API to get secured access to your model, no matter where and how it is deployed.", "Each model comes with a simple monitoring dashboard, helping you to analyze the usage and performance of your model.", "Your model is deployed as a containerized runtime. If required, you can execute the model on your own infrastructure.", "gates comes with a simple monitoring overview for you to monitor basic metrics. {{ className: 'lead' }}\n\nThe y-axis is the metric, the x-axis indicates the last 24 hours (i.e. most-leftern value is _now_, then 1 hour before etc.)\n\n<CaptionedImage\n  src=\"/gates/gates-monitoring-1.png\"\n  title=\"Fig. 1: Monitor the average confidence to see if your model generally performs as you'd expect.\"\n/>\n\n<CaptionedImage\n  src=\"/gates/gates-monitoring-2.png\"\n  title=\"Fig. 2: See if your model is performant, i.e. if you can send it the workload you intend to do.\"\n/>", "gates accesses refinery under the hood to make predictions. {{ className: 'lead' }}\n\nTo enable a secure access anywhere, please first create an access token in [refinery](/refinery).\n\n<CaptionedImage\n  src=\"/gates/refinery-access-token.png\"\n  title=\"Fig. 1: Create a read/write access token in refinery. Make sure to save the token in a safe place.\"\n/>\n\nThe access token should allow your to **read and write**, and you should make sure that you safe the token in a safe place. Once you close the creation modal, you can not see the token again!", "With an access token, you can use selected endpoints of the [workflow](/workflow/workflows) and [store](/workflow/stores) API. {{ className: 'lead' }}", "You can fetch workflows and workflow throughput data from the [workflow API](/workflow/workflows). You can find a Python code snippet for a workflow by clicking on the _View API specs_ button on the top right corner of a workflow canvas.\n\n<CaptionedImage\n  src=\"/workflow/workflow-api-specs.png\"\n  title=\"Fig. 1: You can find a pre-configured snippet to fetch workflow data via Python.\"\n/>\n\nIn this snippet, you need to provide a variable `token`. This is a workflow-specific token, which you can create in the Environments tab. In here, you can create a new access token. In the modal, select the `workflow` type (default), choose the workflow you want to access via API, and then specify if you want to only read from the store, or also write. Further, you must specify an expiration time.\n\n<CaptionedImage\n  src=\"/workflow/workflow-access-token.png\"\n  title=\"Fig. 2: To do so, you must first create a workflow-specific access token.\"\n/>\n\nOnce you click _Create_, you can copy the `token` you require for the API access. **Copy this token into a safe place**. You will not be able to retrieve the token again after closing the modal.\n\n<CaptionedImage\n  src=\"/workflow/workflow-access-token-step2.png\"\n  title=\"Fig. 3: Make sure that you copied the token before you close the modal. You will not be able to see it again in workflow.\"\n/>\n\nYou can now replace the token placeholder in the Python snippet to access the workflow API.", "Similar to the workflow access, you can create an access token for the [store API](/workflow/stores). This will help you to both get existing stores, and push data to them.\n\n<CaptionedImage\n  src=\"/workflow/store-api-specs.png\"\n  title=\"Fig. 4: Similar to workflows, you can find pre-configured Python snippets to fetch data from stores.\"\n/>\n\nYou must authenticate yourself via a store-specific access token, which you can create analagoue to the workflow access token (see above):\n\n<CaptionedImage\n  src=\"/workflow/store-access-token.png\"\n  title=\"Fig. 5: The authentication process is analogue to the one mentioned above.\"\n/>", "Some stores require you to log in via Oauth, a mechanism to allow an application like ours to access resources hosted by other web applications (e.g. your Google inbox) on behalf of a user (you). {{ className: 'lead' }}\nAuth0 has a great [blog post](https://auth0.com/intro-to-iam/what-is-oauth-2) covering the topic in greater detail.\n\nIn workflow, you can grant Oauth access while adding specific stores, such as the \"GMail Read\" store. Once you granted access, these identities can also be seen in the respective page:\n\n<CaptionedImage\n  src=\"/workflow/identity-mgmt.png\"\n  title=\"Fig. 1: You can see which identities you've used to grant workflow access to work with 3rd party applications.\"\n/>\n\nYou don't have to manage anything in here. However, if you want to remove workflow to have access to a certain identity, you can delete an identity here. **CAUTION**: This will have immediate impact on all stores using the identity. Make sure that you don't delete a store that still actively uses an identity in a production environment.", "workflow is the orchestration layer for your natural language-driven tasks. It allows you to build complex workflows, which can be triggered by a variety of events. For instance, you can use workflow to grab data from a 3rd party API, build an NLP in [refinery](/refinery) and [gates](/gates), and then use the results in any further step of the pipeline. {{ className: 'lead' }}\n\n<CaptionedImage\n  src=\"/workflow/complex-workflows.png\"\n  title=\"Fig. 1: Screenshot of workflow's canvas.\"\n/>", "The documentation is structured by features of workflow. If a feature is accessible in some extent via API, there is a small <span className='font-mono text-[0.625rem] font-semibold leading-6'>API</span> badge next to it. In this case, you can find the API specs at the bottom of the page.", "Combined with refinery and gates, workflow is quite powerful for a set of featured use cases. To get a better feeling, we added some sample use cases in here:\n\n- [Building training data for NLP](/use-cases/training-data)\n- [Automating emails](/use-cases/email-channels)\n- [Webscraping](/use-cases/webscraping)\n- [NLP ETL](/use-cases/etl)", "workflow comes with the following features.", "Simple drag-and-drop interface to build your [workflows](/workflows), connected to catalogue of nodes with either no-code or programmable interfaces. This is as intuitive as it gets.", "workflow stands on the shoulders of [refinery](/refinery) and [gates](/gates). This means that workflow is capable of handling the most complex NLP applications (and of course also the simpler ones), while still being easy to use.", "workflow offers native integrations to e.g. Google workspace applications, Slack or other collaboration channels, and further offers an API and Webhook links. We are continuously adding new integrations.", "Data is being stored in [stores](/stores), such that you have direct access to your data source integrations. For instance, you can just export all emails from your inbox into refinery via the GMail integration, and then start building your NLP automations.", "workflow is designed to work out of the box for the use cases you want to implement, whether it is realtime or batch processing. You can run operational tasks in realtime, and batch processing can be used for e.g. data analysis. Alternatively, you can synchronize refinery in a batch-job, such that your training data is always up-to-date. You will quickly realize: This is ETL for NLP - and it is designed to fit your ideas.", "This guide will give you a simple overview of what you can do in workflow. {{ className: 'lead' }}\n\n<Note>\n  To go into deeper detail, please look into the further linked pages along the\n  quickstart.\n</Note>", "This application is called workflow for a reason \ud83d\ude09. The first page you will see is the overview of your current running [workflows](/workflow/workflows), and it is empty. Let's change that. Create a new project from scratch:\n\n<CaptionedImage\n  src=\"/workflow/creation.png\"\n  title=\"Fig. 1: Create a workflow from scratch.\"\n/>\n\nAs you can already see, workflow offers both batch and realtime workflows. We'll dive into the difference on another page. For now, we create a realtime job that is _executed every 5 seconds_.\n\nYou will be forwarded to the workflow details page, where a large empty canvas welcomes you.\n\n<CaptionedImage\n  src=\"/workflow/canvas.png\"\n  title=\"Fig. 2: The workflow canvas.\"\n/>\n\nIn here, we'll be able to implement the steps of a workflow and then connect them. For this, we will need some kind of data source - which is why we're now jumping into the [stores](/workflow/stores) catalogue.", "In the [stores](/workflow/stores) catalogue, you will see a variety of options to choose from. These options are divided into being either a `source`, `target`, or `modifier` (with one exception, the \"Shared Store\", which is both a `source` and `target`).\n\n- `source`: you're pulling raw data from these stores. That could be e.g. a Google spreadsheet you want to process.\n- `target`: you're pushing transformed data into these stores. A target store contains the logic to process incoming records with the integration-specific logic, i.e. pushing data to the \"GMail Send\" store will categorize your inbox, or it will create drafts for you.\n- `modifier`: use them as a step in your workflow to enrich records via lookup tables.\n\n<CaptionedImage\n  src=\"/workflow/store-catalogue.png\"\n  title=\"Fig. 3: The store catalogue in workflow offers you to e.g. integrate different 3rd party applciations.\"\n/>\n\nYou will find further information about the stores in the [stores](/workflow/stores) page. For now, let's select a simple \"Workflow Read\" store, which enables us to enter data manually from the UI. To do so, simply click on _Add store_, which will forward you to the store-specific creation page. In this case, we can create a simple schema for our store. Let's keep it simple:\n\n<CaptionedImage\n  src=\"/workflow/store-add.png\"\n  title=\"Fig. 4: Every store comes with a unique page to configure and add instances.\"\n/>\n\nNow, we can go to this store and click on _Enter data_, which will forward us to the \"Workflow Read\"-specific data entry page. Let's create some sample data:\n\n<CaptionedImage\n  src=\"/workflow/store-enter-data.png\"\n  title=\"Fig. 5: For some stores, you can enter data manually via a UI (or programmatically via API).\"\n/>\n\nThis data is now in our store:\n\n<CaptionedImage\n  src=\"/workflow/store-details.png\"\n  title=\"Fig. 6: You can look into every entry of a store via the details page.\"\n/>\n\nGreat! We have a very simple data source for now, and can jump back into our workflow from before.\n\n<Note>\n  To get an overview of the existing store options, take a look into the\n  [stores](/workflow/stores) page.\n</Note>", "Finally, our step to build our first workflow, we are back in our canvas. We can now use the data store we just created by hitting `\u2318 + K` on the keyboard (or selecting _Add node_ in the bottom left control panel), which will open up the node palette:\n\n<CaptionedImage\n  src=\"/workflow/node-catalogue.png\"\n  title=\"Fig. 7: From the node palette, you can choose a variety of ready-made automations.\"\n/>\n\nThis palette contains the nodes we can use for our workflow. Simply scroll down in this palette to `sources` and choose \"Workflow Read\". You will now see the \"Workflow Read\" node in the canvas, and by clicking on it, you can open the configuration in order to pick the \"Workflow Read\" store we created before.\n\n<Note>\n  Stores that you create are by default called like the name of the store option\n  in the catalogue, i.e. inboxes you connect via \"GMail Read\" are called \"GMail\n  Read\". You can change the name of the store anytime in the details page of\n  your store. More about this on the [stores](/workflow/stores) page.\n</Note>\n\n<CaptionedImage\n  src=\"/workflow/store-select.png\"\n  title=\"Fig. 8: Choose from a store of the 'Workflow Read' integration.\"\n/>\n\nworkflow automatically pulls data from this store for this specific workflow in the frequency you selected during the workflow creation. This means that our \"My project\"-workflow will automatically fetch data from the store _every 5 seconds_. If you're wondering how that fetching mechanism works in detail, please look into the [workflows](/workflow/workflows) page.\n\nWe can now attach this store to another node which consumes the provided data, e.g. a simple Python node to calculate some attribute. Simply open the node catalogue, pick a \"Python\" node and start implementing the code:\n\n<CaptionedImage\n  src=\"/workflow/node-config.png\"\n  title=\"Fig. 9: Configure your node; there are fully programmable Python nodes, and ready-made no-code solutions.\"\n/>\n\nPlease make sure that you implement against the interface of the Python node in that each programmable node takes as input a `record: dict` and outputs either a single `dict` (\"Python\") or a Generator of `dict` (\"Python yield\").\n\n<CodeGroup>\n\n```python\ndef node(record:dict) -> dict:\n  # please keep the name of the function as it is\n\n  # you can compute any custom logic in here\n  my_field = \"some_value\"\n  return {\n    **record,\n    \"my_field\": my_field\n  }\n```\n\n</CodeGroup>\n\nTo forward the data from the \"Workflow Read\" node into the \"Python\" node, you can create a Python condition in the configuration of the \"Workflow Read\" node.\n\n<CaptionedImage\n  src=\"/workflow/node-connect.png\"\n  title=\"Fig. 10: Connect your node with other nodes to create a flow. The conditions for flows can be fully programmed.\"\n/>\n\nThis condition can be anything you'd write as an `if`-condition in Python, even complex conditions such as\n\n```python\nrecord[\"confidence\"] > 0.5 and (record[\"source\"] = \"source_1\" or record[\"count\"] % 3 == 0)\n```\n\nThose are the basic concept of building workflows:\n\n- Creating nodes, either from a store or with some Python logic. A special node in the platform is the `gates AI` node, which we cover in another section.\n- You can use any Python logic to connect the nodes.\n- In this sense, you can build workflows such as [data source -> transformer -> data sink].", "Great, you now have a first understanding of what workflow can do. Here are a few links that might be handy as you venture further into workflow\n\n- [Dive into workflows](/workflow/workflows)\n- [See how to populate stores](/workflow/stores)\n- [Join our Discord if you have any questions](https://discord.com/invite/qf4rGCEphW)", "<Note>\n  If you want to see the API specs, click\n  [here](/workflow/stores#the-store-model).\n</Note>\n\nStores are a central entity in workflow. You can use them to collect data, to modify records in existing workflows via lookup mappings, and you can use them to send data to other applications. We'll dive into greater detail in this section. {{\n  className: 'lead',\n}}", "A store belongs to at minimum one of these three categories:\n\n- `sources`: you're collecting data. That could be e.g. a Google spreadsheet you want to process, or the Workflow API which you can post data to.\n- `targets`: you're sending data to other applications. A target store contains the logic to process incoming records with the integration-specific logic, i.e. pushing data to the \"GMail Send\" store will categorize your inbox, or it will create drafts for you.\n- `modifiers`: use them as a step in your workflow to enrich records via lookup tables.\n\nYou can see all existing store options in the store catalogue:\n\n<CaptionedImage\n  src=\"/workflow/store-catalogue.png\"\n  title=\"Fig. 1: workflow comes with a store catalogue consisting of a set of integrations.\"\n/>", "Each store has its own logic to be added; generally, you can click on _Add store_ if you want to add one. The logic to add a store ranges from authenticating Google via Oauth up to creating a schema. In our example here, we create a \"Workflow Read\" store by defining which fields we'd like to have in the store:\n\n<CaptionedImage\n  src=\"/workflow/store-add.png\"\n  title=\"Fig. 2: Every store has a custom page to create configurations and add instances.\"\n/>", "For every store, you can look into the current entries:\n\n<CaptionedImage\n  src=\"/workflow/store-details.png\"\n  title=\"Fig. 3: You can view the store entries in the store details page.\"\n/>\n\nIn some cases, you can enter data manually:\n\n<CaptionedImage\n  src=\"/workflow/store-enter-data.png\"\n  title=\"Fig. 4: Some stores have the option to manually enter data via UI (or programmatically via API).\"\n/>\n\nWhat options you have in a store is highly depending on the integration used to build that store. For the \"Workflow Read\" store, you can also post data to the store via API (see also [below](/workflow/stores#the-store-entry-model)):\n\n<CaptionedImage\n  src=\"/workflow/store-api-specs.png\"\n  title=\"Fig. 5: You can view the pre-built Python script to fetch the store entries in the _View API specs_ button.\"\n/>", "One of the key benefits of using workflow is that you can synchronize stores with [refinery](/refinery). For instance, if you have an inbox you want to use to steadily grow your database, you can simply set up a refinery project from the store itself.\n\n<CaptionedImage\n  src=\"/workflow/store-refinery-sync.png\"\n  title=\"Fig. 6: You can sychronize your workflow store with a refinery project, such that existing and new entries are automatically added to your refinery project.\"\n/>\n\nOnce you set up a synchronization, a batch job will be started to push the current store data to a newly created refinery project. Afterwards, new incoming data is directly sent to the refinery project and appended to the database.", "You can easily retrieve stores from workflow via their unique ID and an access token you create in the application. This is helpful if you want to monitor the size of your stores programmatically, or if you want to add new entries to the API. {{ className: 'lead' }}", "The store model contains informations such as the name, integration, and the entries.", "<Properties>\n  <Property name=\"id\" type=\"string\">\n    Unique identifier for the store.\n  </Property>\n  <Property name=\"organizationId\" type=\"string\">\n    Unique identifier of the organization in which the store exists.\n  </Property>\n  <Property name=\"name\" type=\"string\">\n    The name of the store itself.\n  </Property>\n  <Property name=\"integration\" type=\"string\">\n    The name of the integration via which you created the store (from the store\n    catalogue).\n  </Property>\n  <Property name=\"icon\" type=\"string\">\n    The unique name of the icon to display the store.\n  </Property>\n  <Property name=\"configSetup\" type=\"dict\">\n    The configuration you provided during the initiation of the store.\n  </Property>\n  <Property name=\"configActions\" type=\"dict\">\n    If this is a store which triggers specific configurable actions, this\n    dictionary will contain the information what to do (e.g. draft messages for\n    the GMail Send store).\n  </Property>\n  <Property name=\"refineryProjectId\" type=\"string\">\n    If this store is synchronized with a project in refinery, it will contain\n    its unique identifier.\n  </Property>\n  <Property name=\"entries\" type=\"list[store-entry]\">\n    Contains N entries of your store.\n  </Property>\n  <Property name=\"numEntries\" type=\"int\">\n    Number of entries in your store.\n  </Property>\n  <Property name=\"numPages\" type=\"int\">\n    Number of pages in your store, i.e. ceil of numEntries // pageSize.\n  </Property>\n  <Property name=\"pageSize\" type=\"int\">\n    Maximum number of entries in this page.\n  </Property>\n</Properties>", "The store entry contains information about the storeId, its data and the timestamp when it was created.", "<Properties>\n  <Property name=\"id\" type=\"string\">\n    Unique identifier for the store entry.\n  </Property>\n  <Property name=\"storeId\" type=\"string\">\n    Unique identifier for the store.\n  </Property>\n  <Property name=\"record\" type=\"dict\">\n    The actual store entry data.\n  </Property>\n  <Property name=\"timestamp\" type=\"timestamp\">\n    Timestamp of when the store entry was created.\n  </Property>\n</Properties>\n\n---", "<Row>\n  <Col>\n\n    This endpoint allows you to retrieve a store by providing their id. Refer to [the list](#the-store-model) at the top of this page to see which properties are included with store objects.\n\n    ### Optional attributes\n\n    <Properties>\n      <Property name=\"page\" type=\"int\">\n        Which page to load from the store.\n      </Property>\n    </Properties>\n\n  </Col>\n  <Col sticky>\n\n    <CodeGroup title=\"Request\" tag=\"GET\" label=\"/workflow-api/stores/WAz8eIbvDR60rouK?page=1\">\n\n    ```bash {{ title: 'cURL' }}\n    curl https://app.kern.ai/workflow-api/stores/WAz8eIbvDR60rouK?page=1 \\\n      -H \"Authorization: {token}\"\n    ```\n\n\n    ```python\n    import requests\n\n    my_token = \"INSERT YOUR TOKEN\"\n\n    response = requests.get(\n      \"https://app.kern.ai/workflow-api/stores/WAz8eIbvDR60rouK?page=1\",\n      headers = {\"Authorization\": my_token}\n    )\n    ```\n\n    </CodeGroup>\n\n    ```json {{ title: 'Response' }}\n    {\n      \"id\": \"1e844bf2-5ebb-40a5-8aa4-7e6b5e79479a\",\n      \"organizationId\": \"8502b0fb-cfaf-45ce-b0a7-71231cb8e769\",\n      \"name\": \"My store\",\n      \"integration\":\n      \"Workflow Read\",\n      \"description\": \"Reads data from an API or a UI\",\n      \"icon\": \"workflow\",\n      \"entries\": [{\n        \"id\": \"87e71284-4c3f-46a1-a58c-55ccc08c7349\",\n        \"storeId\": \"1e844bf2-5ebb-40a5-8aa4-7e6b5e79479a\",\n        \"record\": {\"field_1\": \"This is an example!\"},\n        \"timestamp\": \"2023-02-13T20:22:03.260008\"\n        }, {\n          \"id\": \"6e86b3ad-5436-4c23-bfb2-036216166c76\",\n          \"storeId\": \"1e844bf2-5ebb-40a5-8aa4-7e6b5e79479a\",\n          \"record\": {\"field_1\": \"Hello World\"},\n          \"timestamp\": \"2023-02-13T20:21:59.775006\"\n      }],\n      \"numEntries\": 2,\n      \"numPages\": 1,\n      \"pageSize\": 25,\n      \"configSetup\": {\n        \"config\": {\"field_1\": \"Text\"}\n      },\n      \"configActions\": {},\n      \"state\": \"RUNNING\",\n      \"refineryProjectId\": \"None\"\n    }\n\n\n    ```\n\n  </Col>\n</Row>\n\n---", "<Row>\n  <Col>\n\n    This endpoint allows you to post entries to your store. Note: If the record doesn't fit the expected schema, the API will return a 400 status code.\n\n  </Col>\n  <Col sticky>\n\n    <CodeGroup title=\"Request\" tag=\"POST\" label=\"workflow-api/stores/WAz8eIbvDR60rouK/add-entry\">\n\n    ```bash {{ title: 'cURL' }}\n    curl -X DELETE https://app.kern.ai/workflow-api/stores/WAz8eIbvDR60rouK/add-entry \\\n      -H \"Authorization: {token}\"\n    ```\n\n    ```python\n    import requests\n\n    my_token = \"INSERT YOUR TOKEN\"\n    my_data = {\"key1\": \"value1\"}\n\n    response = requests.get(\n      \"https://app.kern.ai/workflow-api/stores/WAz8eIbvDR60rouK/add-entry\",\n      headers = {\"Authorization\": my_token},\n      json=my_data\n    )\n    ```\n\n    </CodeGroup>\n\n    ```json {{ title: 'Response' }}\n    {\n      \"id\": \"70ecd4ad-9270-43ea-b439-5ee628833016\",\n      \"storeId\": \"1e844bf2-5ebb-40a5-8aa4-7e6b5e79479a\",\n      \"record\": {\"field_1\": \"Text\"},\n      \"timestamp\": \"2023-02-13T20:25:57.822324\"\n    }\n    ```\n\n  </Col>\n</Row>", "You can create environment-like variables in workflow. This is helpful if you have shared settings or things like usernames you want to manage in a central place. {{ className: 'lead' }}\n\nTo create a variable, simply type in a name, value and description.\n\n<CaptionedImage\n  src=\"/workflow/env-variable.png\"\n  title=\"Fig. 1: You can create some environment variables to keep your code clean and easily maintainable.\"\n/>\n\nAfterwards, you can use that environment variable in a node using curly braces `{{ }}`, e.g. `{{ my_variable }}`.\n\n<CaptionedImage\n  src=\"/workflow/node-my-variable.png\"\n  title=\"Fig. 2: Once created, you can use curly braces to access the variables.\"\n/>", "<Note>\n  If you want to see the API specs, click\n  [here](/workflow/workflows#the-workflow-model).\n</Note>\n\nThis product (workflow) is named after its core entity, i.e. workflows. A workflow defines steps that are executed based on some frequency, e.g. _every second_. For instance, a workflow to pull data from an inbox, which is then processed via a [gates](/gates) endpoint, and ultimately fed back for categorization or similar.", "In workflow, you can define two types of processess: `realtime` (i.e. operational) and `batch` (i.e. analytical) workflows. They differ in a very simple way:\n\n- `realtime`: are executed per item, and can be executed e.g. every second\n- `batch`: technically also work on item-level, but they are executed on batches of data, and can be executed e.g. every day. This higher level of granularity makes them perfect for analytical jobs.\n\n<CaptionedImage\n  src=\"/workflow/batch-jobs.png\"\n  title=\"Fig. 1: You can either build your workflow as a realtime or batched job.\"\n/>\n\nWhen to choose which depends on your use case. In most cases, starting with a `realtime` job makes perfect sense.", "The canvas is your drag-and-drop editor in workflow.\n\n<CaptionedImage\n  src=\"/workflow/canvas.png\"\n  title=\"Fig. 2: The workflow canvas.\"\n/>\n\nIn this canvas, you can hit `\u2318 + K` or hit the _Add node_ button in the left bottom control section. This will open up a scrollable and filterable node palette for you.\n\n<CaptionedImage\n  src=\"/workflow/node-catalogue.png\"\n  title=\"Fig. 3: You can choose from a wide set of ready-made automations in the node palette.\"\n/>", "The node palette is divided into three categories: `sources`, `modifiers` and `targets`. You can find more details about these categories on the [stores](/workflow/stores) page. We will cover each category with an example now.", "A source grabs data from a store that you've set up (if you are not sure how to do so, look into the [store](/workflow/stores) page). For instance, you can set up a \"Workflow Read\" store, and then use the \"Workflow Read\" node.\n\n<CaptionedImage\n  src=\"/workflow/store-select.png\"\n  title=\"Fig. 4: Pick a store from the options you previously created.\"\n/>\n\n<Note>\n  If a node has a red border, it means that you haven't configured the node yet.\n  Simply click on it, and finish its configuration (most of the times this just\n  means selecting a store).\n</Note>\n\nWhen you fetch data from a store node during a workflow, a marker will be set in the database which remembers to not pull the entry anymore for this specific workflow; in other words, every entry in a store node will only be fetched once by a workflow. As this marker is store-workflow-specific, it means that you can pull the same entry from a store in multiple workflows. If you _Reset_ the workflow, all markers for this store-workflow-combination will be removed, meaning you can pull all entries again.\n\nIn other words: when you're developing a workflow and noticed that you made a mistake in some Python logic, you can simply reset the workflow and start over. **Caution**: Data that you send to a store as a result of a workflow will not be removed by hitting _Reset_!", "A modifier node is inbetween two other nodes in a workflow; it takes a record (`:dict`) as input, and returns either exactly one record, or yields an arbitrary amount of records. A simple example is the \"Python\" node.\n\n<CaptionedImage\n  src=\"/workflow/node-config.png\"\n  title=\"Fig. 5: Configure your node either via a Python script or via simple no-code configurations.\"\n/>\n\nAlternatively, you can also use a loop of the \"Python\" node, which comes in handy when you e.g. want to webscrape a page and yield multiple records per scraped page:\n\n<CaptionedImage\n  src=\"/workflow/node-loop.png\"\n  title=\"Fig. 6: workflow comes also with complex mechanisms such as the 'Python yield' node, which allows you to emit multiple records.\"\n/>", "A target node is triggering an action when a new entry is put into it. For instance, you can set up a \"GMail Send\" store which tries to create a draft for an existing message, and which categorizes emails based on some previous logic. To make sure that no action is triggered twice, all `target` nodes expect a unique `id`. In simplest cases, this can be something like a `uuid`, but generally it must be unique. If a store requires other fields (e.g. `draft` or `category`, it will tell you so in the configuration screen).\n\n<CaptionedImage\n  src=\"/workflow/target-node.png\"\n  title=\"Fig. 7: When sending data to a target node, make sure that your records fulfill the schema criteria.\"\n/>\n\nWhich actions are triggered by the store can occasionally be configured in the store creation page. For instance, for the \"GMail Send\" node, you can define whether you rather want to draft messages, or directly send messages.\n\n<CaptionedImage\n  src=\"/workflow/store-action.png\"\n  title=\"Fig. 8: Pick from different store actions when creating your `target` store.\"\n/>", "[gates](/gates) is natively integrated in workflow, and you can use any gates model that is running as a `modifier` node.\n\n<CaptionedImage\n  src=\"/workflow/node-gate-selection.png\"\n  title=\"Fig. 9: gates models are automatically synchronized with workflow, such that you can easily pick them here.\"\n/>", "The way to connect nodes in workflow is to write Python conditions. Something you would write immediately after an `if` keyword, e.g. `if record[\"confidence\"] > 0.8:` would be `record[\"confidence\"] > 0.8` in a node connection. If you want to forward data in any case, just type `True` into the node connection.\n\n<CaptionedImage\n  src=\"/workflow/node-connect.png\"\n  title=\"Fig. 10: You can program the conditions that should be met for a node to send data to the next node\"\n/>\n\n<Note>\n  We're drawing edges between nodes on a simple logic:\n  <ul>\n    <li>\n      If a condition is `True`, we draw a solid line. We don't evaluate the\n      condition in advance, i.e. this is only the case if the box contains the\n      actual word `True` and nothing else.\n    </li>\n    <li>\n      If a condition is `False`, we don't draw a line at all. Again, this is a\n      hard string-match.\n    </li>\n    <li>For everything else, we draw a dashed line.</li>\n  </ul>\n\n    Technically, you could draw a dashed line by entering `1` into the conditional field, which logically for Python is equivalent as `True` for conditions. But we encourage you to simply use `True` :)\n\n</Note>\n\nYou can also create a node via our quick connection modal. Click on the bottom right icon of a node container to open the modal:\n\n<CaptionedImage\n  src=\"/workflow/conditional-edge.png\"\n  title=\"Fig. 11: You can also create edge conditions from the quick modal of a node.\"\n/>", "You can run a workflow by clicking _Run_ in the bottom left command palette. Every running workflow is encapsuled in a Docker container. If you hit _Run_, a container will be activated within few seconds.", "Inside a \"Python\" node, you can write some simple `print` logic to log what your node is doing. In addition, you can also look into what output your node creates. This is especially helpful while you are implementing a workflow, as you can gradually look into results of a node before attaching it to a final `target` store.\n\n<CaptionedImage\n  src=\"/workflow/node-log.png\"\n  title=\"Fig. 12: For every node, you can create a log to investigate what happens in your workflow.\"\n/>", "If you created an [environment variable](/workflow/variables), you can now use it inside of your Python script. For instance, if your variable name is `my_variable`, you can simply use it in a script with `{{ my_variable }}`.\n\n<CaptionedImage\n  src=\"/workflow/node-my-variable.png\"\n  title=\"Fig. 13: You can access environment variables from within a node to encapsulate variable settings.\"\n/>\n\nThis, in turn, would create the following log (`my_variable` is `\"Hello from my environment variable\"` in this case):\n\n<CaptionedImage\n  src=\"/workflow/node-variable-log.png\"\n  title=\"Fig. 14: To validate that you correctly set up your variable, you can simply print its output.\"\n/>", "Ultimately, you can build quite complex workflows with the editor. The following is an example of a workflow that can be read as follows:\n\n- a `source` node fetches data from a GMail inbox\n- each message is enriched via a gates AI node, e.g. to categorize the message or to extract named entities (see more in [refinery](/refinery) and [gates](/gates))\n- some postprocessing Python logic, e.g. to flatten the structure of the output\n- conditional forwards, e.g. depending on the detected intent or complexity of the message. Either an automated message is sent, or an operator is notified on Slack.\n\n<CaptionedImage\n  src=\"/workflow/complex-workflows.png\"\n  title=\"Fig. 15: You can both configure simple or more complex, multi-step workflows. Workflows can also be spread across different workflow projects, e.g. (1) webscraping, (2) enriching, and (3) aggregating values.\"\n/>", "You can monitor the throughput of a workflow on the monitoring page. The throughput is how many tasks have been initiated in a workflow, i.e. as soon as a record has been fetched from a `source` store, it counts as one task.\n\n<CaptionedImage\n  src=\"/workflow/workflow-monitoring.png\"\n  title=\"Fig. 16: You can monitor the throughput of your workflow.\"\n/>", "You can easily retrieve workflows via their unique ID and an access token you create in the application. This is helpful if you want to monitor the throughput of your workflows programmatically. {{ className: 'lead' }}", "The store model contains informations such as the name, state, and the nodes.", "<Properties>\n  <Property name=\"id\" type=\"string\">\n    Unique identifier for the workflow.\n  </Property>\n  <Property name=\"organizationId\" type=\"string\">\n    Unique identifier of the organization in which the workflow exists.\n  </Property>\n  <Property name=\"name\" type=\"string\">\n    The name of the workflow itself.\n  </Property>\n  <Property name=\"description\" type=\"string\">\n    The description of the workflow.\n  </Property>\n  <Property name=\"state\" type=\"string\">\n    Whether the workflow is currently running or in draft state.\n  </Property>\n  <Property name=\"refreshRateInSeconds\" type=\"int\">\n    How many seconds need to pass by until the workflow is triggered again.\n  </Property>\n  <Property name=\"executionType\" type=\"string\">\n    Either realtime or batch.\n  </Property>\n  <Property name=\"color\" type=\"string\">\n    Color of the dot in the workflow overview page.\n  </Property>\n  <Property name=\"nodes\" type=\"list[node]\">\n    List of all nodes in the workflow.\n  </Property>\n  <Property name=\"nodeConnections\" type=\"dict\">\n    Dictionary with the conditions for nodes to connect.\n  </Property>\n  <Property name=\"containerIsAvailable\" type=\"bool\">\n    Each workflow is running in a containerized environment. If this is `false`,\n    it means that the workflow can't be executed.\n  </Property>\n</Properties>", "The node contains information about the workflowId, its implementation and the conditions between nodes when it was created.", "<Properties>\n  <Property name=\"id\" type=\"string\">\n    Unique identifier for the node.\n  </Property>\n  <Property name=\"organizationId\" type=\"string\">\n    Unique identifier of the organization in which the workflow exists.\n  </Property>\n  <Property name=\"name\" type=\"string\">\n    The name of the node itself.\n  </Property>\n  <Property name=\"description\" type=\"string\">\n    The description of the node.\n  </Property>\n  <Property name=\"category\" type=\"string\">\n    Either `sources`, `modifiers` or `targets`\n  </Property>\n  <Property name=\"code\" type=\"string\">\n    Python implementation of the node. For no code nodes, this contains\n    placeholders which are replaced before executing the node.\n  </Property>\n  <Property name=\"isReturnType\" type=\"bool\">\n    In most cases, this is `true`. It is `false` for the \"Python yield\" node,\n    which doesn't use the Python `return` keyword, but instead the `yield`\n    keyword.\n  </Property>\n  <Property name=\"isNoCode\" type=\"bool\">\n    If `true`, the code attribute contains placeholders that must be replaced\n    before execution.\n  </Property>\n  <Property name=\"noCodeValues\" type=\"dict\">\n    Containing the replacements for the placeholders.\n  </Property>\n  <Property name=\"flowDict\" type=\"dict\">\n    Containing the condition logic for forwarding data in a workflow.\n  </Property>\n  <Property name=\"positionX\" type=\"int\">\n    Relative horizontal position on the canvas.\n  </Property>\n  <Property name=\"positionY\" type=\"int\">\n    Relative vertical position on the canvas.\n  </Property>\n  <Property name=\"icon\" type=\"string\">\n    Icon identifier for resolving the icon image.\n  </Property>\n  <Property name=\"useCase\" type=\"string\">\n    Name of the underlying use case (relevant for templates)\n  </Property>\n  <Property name=\"isTemplate\" type=\"bool\">\n    If `true`, this node itself can't be executed. It can be replaced with an\n    actual node.\n  </Property>\n  <Property name=\"fromStore\" type=\"string\">\n    If a node was initialized from a store, this contains the name of the\n    integration.\n  </Property>\n  <Property name=\"isReady\" type=\"bool\">\n    If `true`, the node can be executed.\n  </Property>\n</Properties>\n\n---", "<Row>\n  <Col>\n\n    This endpoint allows you to retrieve a workflow by providing their id. Refer to [the list](#the-workflow-model) at the top of this page to see which properties are included with workflow objects.\n\n  </Col>\n  <Col sticky>\n\n    <CodeGroup title=\"Request\" tag=\"GET\" label=\"/workflow-api/workflows/WAz8eIbvDR60rouK\">\n\n    ```bash {{ title: 'cURL' }}\n    curl https://app.kern.ai/workflow-api/workflows/WAz8eIbvDR60rouK \\\n      -H \"Authorization: {token}\"\n    ```\n\n\n    ```python\n    import requests\n\n    my_token = \"INSERT YOUR TOKEN\"\n\n    response = requests.get(\n      \"https://app.kern.ai/workflow-api/workflows/WAz8eIbvDR60rouK\",\n      headers = {\"Authorization\": my_token}\n    )\n    ```\n\n    </CodeGroup>\n\n    ```json {{ title: 'Response' }}\n    {\n      \"id\": \"f5820b73-0a07-4551-b2f3-31486a124188\",\n      \"organizationId\": \"8502b0fb-cfaf-45ce-b0a7-71231cb8e769\",\n      \"name\": \"My project\",\n      \"description\": \"This is a description of my project\",\n      \"state\": \"RUNNING\",\n      \"refreshRateInSeconds\": 5,\n      \"executionType\": \"REALTIME\",\n      \"color\": \"bg-red-500\",\n      \"nodes\": [{\n        \"id\": \"380c8c39-c333-4e41-8d40-adb63972f826\", \"\n        organizationId\": \"8502b0fb-cfaf-45ce-b0a7-71231cb8e769\",\n        \"workflowId\": \"f5820b73-0a07-4551-b2f3-31486a124188\",\n        \"name\": \"Auto Trigger\",\n        \"description\": \"Emits a timestamp trigger\",\n        \"category\": \"sources\",\n        \"code\": \"def node() -> dict:\\n    import time\\n\\n    return {\\n        \"unix_timestamp\": time.time(),\\n    }\\n\",\n        \"isReturnType\": True,\n        \"isAggregatorType\": False,\n        \"isNoCode\": True,\n        \"noCodeValues\": {},\n        \"flowDict\": {\n          \"5057c850-31e4-4f64-825f-e2ad5774fff3\": \"True\",\n          \"cc79a21f-c36a-4f8e-887c-07bf0cc2f131\": \"False\"\n        },\n        \"positionX\": 75,\n        \"positionY\": 75,\n        \"icon\": \"CRON\",\n        \"useCase\": \"Auto Trigger\",\n        \"isTemplate\": False,\n        \"fromStore\": None,\n        \"isReady\": True\n      }, {\n        \"id\": \"5057c850-31e4-4f64-825f-e2ad5774fff3\",\n        \"organizationId\": \"8502b0fb-cfaf-45ce-b0a7-71231cb8e769\",\n        \"workflowId\": \"f5820b73-0a07-4551-b2f3-31486a124188\",\n        \"name\": \"Python\",\n        \"description\": \"Modify your data using Python\",\n        \"category\": \"modifiers\",\n        \"code\": \"def node(record: dict):\\n    from datetime import datetime\\n    from uuid import uuid4\\n\\n    processed_at = datetime.now().isoformat()\\n    print(f\"Processing record at {processed_at}\")\\n\\n    return {\\n        \"id\": str(uuid4()),\\n        **record,\\n    }\\n\",\n        \"isReturnType\": True,\n        \"isAggregatorType\": False,\n        \"isNoCode\": False,\n        \"noCodeValues\": {},\n        \"flowDict\": {\n          \"380c8c39-c333-4e41-8d40-adb63972f826\": \"False\",\n          \"cc79a21f-c36a-4f8e-887c-07bf0cc2f131\": \"True\"\n        },\n        \"positionX\": 150,\n        \"positionY\": 202,\n        \"icon\": \"python\",\n        \"useCase\": None,\n        \"isTemplate\": False,\n        \"fromStore\": None,\n        \"isReady\": True\n      }, {\n        \"id\": \"cc79a21f-c36a-4f8e-887c-07bf0cc2f131\",\n        \"organizationId\": \"8502b0fb-cfaf-45ce-b0a7-71231cb8e769\",\n        \"workflowId\": \"f5820b73-0a07-4551-b2f3-31486a124188\",\n        \"name\": \"Shared Store Send\",\n        \"description\": \"Send data to a shared store\",\n        \"category\": \"targets\",\n        \"code\": \"def node(record):\\n    import requests\\n\\n    response = requests.post(\\n        url=\"http://workflow-engine:80/stores/@@PLACEHOLDER_MART_ID@@/create-entry-from-node\",\\n        headers={\"Content-Type\": \"application/json\"},\\n        json={\\n            \"nodeId\": \"@@PLACEHOLDER_NODE_ID@@\",\\n            \"data\": record,\\n        },  # you might need to change this to the format of the webhook\\n    )\\n    if response.status_code == 200:\\n        data = response.json()\\n        print(f\"Sending {data} to store\")\\n\",\n        \"isReturnType\": True,\n        \"isAggregatorType\": False,\n        \"isNoCode\": True,\n        \"noCodeValues\": {\n          \"PLACEHOLDER_MART_ID\": \"a6d7530f-257f-44cc-a575-f2371c54193e\",\n          \"PLACEHOLDER_DATETIME\": \"2000-01-01T00:00\",\n          \"PLACEHOLDER_EMISSION_TYPE\": \"record\"\n        }, \"flowDict\": {\n          \"380c8c39-c333-4e41-8d40-adb63972f826\": \"False\",\n          \"5057c850-31e4-4f64-825f-e2ad5774fff3\": \"False\"\n        },\n        \"positionX\": 232,\n        \"positionY\": 337,\n        \"icon\": \"workflow\",\n        \"useCase\": None,\n        \"isTemplate\": False,\n        \"fromStore\": \"Shared Store Send\",\n        \"isReady\": True\n      }],\n      \"nodeConnections\": {\n        \"380c8c39-c333-4e41-8d40-adb63972f826\": [],\n        \"5057c850-31e4-4f64-825f-e2ad5774fff3\": [\"380c8c39-c333-4e41-8d40-adb63972f826\"],\n        \"cc79a21f-c36a-4f8e-887c-07bf0cc2f131\": [\"380c8c39-c333-4e41-8d40-adb63972f826\", \"5057c850-31e4-4f64-825f-e2ad5774fff3\"]\n      },\n      \"containerIsAvailable\": True\n    }\n    ```\n\n  </Col>\n</Row>\n\n---", "<Row>\n  <Col>\n\n    This endpoint allows you to retrieve the throughput of a workflow as a simple list.\n\n  </Col>\n  <Col sticky>\n\n    <CodeGroup title=\"Request\" tag=\"POST\" label=\"workflow-api/workflows/WAz8eIbvDR60rouK/throughput\">\n\n    ```bash {{ title: 'cURL' }}\n    curl -X DELETE https://app.kern.ai/workflow-api/workflows/WAz8eIbvDR60rouK/throughput \\\n      -H \"Authorization: {token}\"\n    ```\n\n    ```python\n    import requests\n\n    my_token = \"INSERT YOUR TOKEN\"\n\n    response = requests.get(\n      \"https://app.kern.ai/workflow-api/workflows/WAz8eIbvDR60rouK/throughput\",\n      headers = {\"Authorization\": my_token},\n      json=my_data\n    )\n    ```\n\n    </CodeGroup>\n\n    ```json {{ title: 'Response' }}\n    [{\n      \"timestampMinute\": \"2023-02-13T20:27:00\", \"numTasks\": 5\n    }, {\n      \"timestampMinute\": \"2023-02-13T20:28:00\", \"numTasks\": 4\n    }]\n    ```\n\n  </Col>\n</Row>"], "url": ["http://docs.kern.ai/refinery/adding-attributes#adding-attributes", "http://docs.kern.ai/refinery/adding-attributes#motivation", "http://docs.kern.ai/refinery/adding-attributes#adding-new-attributes-in-refinery", "http://docs.kern.ai/refinery/attribute-visibility#attribute-visibility", "http://docs.kern.ai/refinery/attribute-visibility#visibility-options", "http://docs.kern.ai/refinery/bricks-integration#bricks-integration", "http://docs.kern.ai/refinery/comments#comments", "http://docs.kern.ai/refinery/configuration-page#configuration-page", "http://docs.kern.ai/refinery/data-export#data-export", "http://docs.kern.ai/refinery/data-export#download-records", "http://docs.kern.ai/refinery/data-export#creating-a-project-snapshot", "http://docs.kern.ai/refinery/data-management#data-management", "http://docs.kern.ai/refinery/data-management#filtering", "http://docs.kern.ai/refinery/data-management#attribute-filters", "http://docs.kern.ai/refinery/data-management#labeling-task-filters", "http://docs.kern.ai/refinery/data-management#comment-filter", "http://docs.kern.ai/refinery/data-management#combining-filters", "http://docs.kern.ai/refinery/data-management#from-filters-to-data-slices", "http://docs.kern.ai/refinery/data-management#creating-data-slices", "http://docs.kern.ai/refinery/data-management#types-of-data-slices", "http://docs.kern.ai/refinery/data-management#sharing-data-slices", "http://docs.kern.ai/refinery/data-management#ordering", "http://docs.kern.ai/refinery/data-management#similarity-and-outliers", "http://docs.kern.ai/refinery/data-management#configuration", "http://docs.kern.ai/refinery/data-management#examples-and-best-practices", "http://docs.kern.ai/refinery/data-management#finding-label-mismatches", "http://docs.kern.ai/refinery/embedding-integration#embedding-integration", "http://docs.kern.ai/refinery/embedding-integration#workflow", "http://docs.kern.ai/refinery/embedding-integration#downloaded-models", "http://docs.kern.ai/refinery/evaluating-heuristics#evaluating-heuristics", "http://docs.kern.ai/refinery/evaluating-heuristics#statistics", "http://docs.kern.ai/refinery/evaluating-heuristics#updating-interval", "http://docs.kern.ai/refinery/evaluating-heuristics#best-practices", "http://docs.kern.ai/refinery/evaluating-heuristics#random-labeling", "http://docs.kern.ai/refinery/evaluating-heuristics#validation-slices", "http://docs.kern.ai/refinery/heuristics#heuristics", "http://docs.kern.ai/refinery/heuristics#labeling-functions", "http://docs.kern.ai/refinery/heuristics#creating-a-labeling-function", "http://docs.kern.ai/refinery/heuristics#writing-a-labeling-function", "http://docs.kern.ai/refinery/heuristics#running-a-labeling-function", "http://docs.kern.ai/refinery/heuristics#deleting-a-labeling-function", "http://docs.kern.ai/refinery/heuristics#container-logs", "http://docs.kern.ai/refinery/heuristics#best-practices-and-examples", "http://docs.kern.ai/refinery/heuristics#validation", "http://docs.kern.ai/refinery/heuristics#lookup-lists-for-distant-supervision", "http://docs.kern.ai/refinery/heuristics#heuristics-for-extraction-tasks", "http://docs.kern.ai/refinery/heuristics#active-learners", "http://docs.kern.ai/refinery/heuristics#creating-an-active-learner", "http://docs.kern.ai/refinery/heuristics#writing-an-active-learner", "http://docs.kern.ai/refinery/heuristics#running-an-active-learner", "http://docs.kern.ai/refinery/heuristics#deleting-an-active-learner", "http://docs.kern.ai/refinery/heuristics#best-practices-and-examples", "http://docs.kern.ai/refinery/heuristics#validation", "http://docs.kern.ai/refinery/heuristics#custom-active-learning-model", "http://docs.kern.ai/refinery/heuristics#...", "http://docs.kern.ai/refinery/heuristics#minimum-confidence-for-finetuning", "http://docs.kern.ai/refinery/heuristics#active-learning-for-extraction", "http://docs.kern.ai/refinery/heuristics#zero-shot-classifiers", "http://docs.kern.ai/refinery/heuristics#creating-a-zero-shot-classifier", "http://docs.kern.ai/refinery/heuristics#experimenting-with-the-zero-shot-classifier", "http://docs.kern.ai/refinery/heuristics#running-a-zero-shot-classifier", "http://docs.kern.ai/refinery/heuristics#deleting-a-zero-shot-classifier", "http://docs.kern.ai/refinery/heuristics#examples-and-best-practices", "http://docs.kern.ai/refinery/heuristics#validation", "http://docs.kern.ai/refinery/heuristics#crowd-labeling", "http://docs.kern.ai/refinery/heuristics#pre-requisites", "http://docs.kern.ai/refinery/heuristics#creating-a-crowd-labeling-heuristic", "http://docs.kern.ai/refinery/heuristics#distributing-the-labeling-work", "http://docs.kern.ai/refinery/heuristics#deleting-a-crowd-labeling-heuristic", "http://docs.kern.ai/refinery#refinery", "http://docs.kern.ai/refinery#flagship-product-of-kern-ai", "http://docs.kern.ai/refinery#structure-of-this-documentation", "http://docs.kern.ai/refinery#features-of-refinery", "http://docs.kern.ai/refinery#manual-labeling-editor", "http://docs.kern.ai/refinery#best-in-class-data-management", "http://docs.kern.ai/refinery#native-large-language-model-integration-and-finetuning", "http://docs.kern.ai/refinery#automate-with-heuristics", "http://docs.kern.ai/refinery#monitor-your-data-quality", "http://docs.kern.ai/refinery#open-source", "http://docs.kern.ai/refinery/installation#installation", "http://docs.kern.ai/refinery/installation#from-repository-(recommended)", "http://docs.kern.ai/refinery/installation#with-pip", "http://docs.kern.ai/refinery/installation#accessing-the-ui", "http://docs.kern.ai/refinery/labeling-tasks#labeling-tasks", "http://docs.kern.ai/refinery/labeling-tasks#types-of-labeling-tasks", "http://docs.kern.ai/refinery/labeling-tasks#creating-labeling-tasks", "http://docs.kern.ai/refinery/labeling-tasks#deleting-labeling-tasks", "http://docs.kern.ai/refinery/labeling-tasks#labels", "http://docs.kern.ai/refinery/labeling-tasks#creating-labels", "http://docs.kern.ai/refinery/labeling-tasks#renaming-labels", "http://docs.kern.ai/refinery/labeling-tasks#deleting-labels", "http://docs.kern.ai/refinery/labeling-tasks#quality-of-life", "http://docs.kern.ai/refinery/labeling-tasks#label-colors-and-keyboard-shortcuts", "http://docs.kern.ai/refinery/managed-version#managed-version", "http://docs.kern.ai/refinery/managed-version#signing-up", "http://docs.kern.ai/refinery/managing-roles#managing-roles", "http://docs.kern.ai/refinery/managing-roles#role-system", "http://docs.kern.ai/refinery/managing-roles#user-overview-page", "http://docs.kern.ai/refinery/managing-roles#adding-new-team-members", "http://docs.kern.ai/refinery/manual-labeling#manual-labeling", "http://docs.kern.ai/refinery/manual-labeling#core-concept:-labeling-sessions", "http://docs.kern.ai/refinery/manual-labeling#starting-a-session-from-the-navigation-bar", "http://docs.kern.ai/refinery/manual-labeling#starting-a-session-from-the-data-browser", "http://docs.kern.ai/refinery/manual-labeling#persistence-of-labeling-sessions", "http://docs.kern.ai/refinery/manual-labeling#labeling-workflow", "http://docs.kern.ai/refinery/manual-labeling#creating-labels", "http://docs.kern.ai/refinery/manual-labeling#assigning-labels", "http://docs.kern.ai/refinery/manual-labeling#classification-label", "http://docs.kern.ai/refinery/manual-labeling#information-extraction-label", "http://docs.kern.ai/refinery/manual-labeling#weak-supervision-label", "http://docs.kern.ai/refinery/manual-labeling#deleting-assigned-labels", "http://docs.kern.ai/refinery/manual-labeling#customizing-the-labeling-view", "http://docs.kern.ai/refinery/manual-labeling#weak-supervision-heuristics-and-model-callbacks", "http://docs.kern.ai/refinery/manual-labeling#labeling-suite-settings", "http://docs.kern.ai/refinery/manual-labeling#quality-of-life", "http://docs.kern.ai/refinery/manual-labeling#highlighting", "http://docs.kern.ai/refinery/manual-labeling#record-ide", "http://docs.kern.ai/refinery/model-callbacks#model-callbacks", "http://docs.kern.ai/refinery/monitoring#monitoring", "http://docs.kern.ai/refinery/monitoring#analyzing-label-quality-distribution-and-user-agreement", "http://docs.kern.ai/refinery/monitoring#label-distribution", "http://docs.kern.ai/refinery/monitoring#confidence-distribution", "http://docs.kern.ai/refinery/monitoring#confusion-matrix", "http://docs.kern.ai/refinery/monitoring#inter-annotator-agreement", "http://docs.kern.ai/refinery/monitoring#analyzing-metrics-on-static-slices", "http://docs.kern.ai/refinery/multi-user-labeling#multi-user-labeling", "http://docs.kern.ai/refinery/multi-user-labeling#solving-conflicts-by-selecting-gold-labels", "http://docs.kern.ai/refinery/multi-user-labeling#sharing-labeling-sessions-with-engineers", "http://docs.kern.ai/refinery/multi-user-labeling#labeling-as-an-expert", "http://docs.kern.ai/refinery/multi-user-labeling#labeling-as-an-annotator", "http://docs.kern.ai/refinery/neural-search#neural-search", "http://docs.kern.ai/refinery/neural-search#similarity-search", "http://docs.kern.ai/refinery/neural-search#outlier-detection", "http://docs.kern.ai/refinery/project-creation-and-data-upload#project-creation-&-data-upload", "http://docs.kern.ai/refinery/project-creation-and-data-upload#project-creation-workflow", "http://docs.kern.ai/refinery/project-creation-and-data-upload#data-requirements", "http://docs.kern.ai/refinery/project-creation-and-data-upload#uploading-(partially)-labeled-data", "http://docs.kern.ai/refinery/project-creation-and-data-upload#example-json-for-attribute-level-classification", "http://docs.kern.ai/refinery/project-creation-and-data-upload#example-json-for-full-record-classification", "http://docs.kern.ai/refinery/project-creation-and-data-upload#uploading-more-data-later-during-the-project", "http://docs.kern.ai/refinery/project-creation-and-data-upload#the-`running_id`-attribute", "http://docs.kern.ai/refinery/quickstart#quickstart", "http://docs.kern.ai/refinery/quickstart#set-up-the-clickbait-example-project", "http://docs.kern.ai/refinery/quickstart#inspecting-the-data", "http://docs.kern.ai/refinery/quickstart#understanding-the-project-settings", "http://docs.kern.ai/refinery/quickstart#data-schema", "http://docs.kern.ai/refinery/quickstart#embeddings", "http://docs.kern.ai/refinery/quickstart#labeling-tasks", "http://docs.kern.ai/refinery/quickstart#labeling", "http://docs.kern.ai/refinery/quickstart#pattern-discovery", "http://docs.kern.ai/refinery/quickstart#writing-our-first-labeling-function", "http://docs.kern.ai/refinery/quickstart#leveraging-lookup-lists", "http://docs.kern.ai/refinery/quickstart#leveraging-embeddings-with-active-learners", "http://docs.kern.ai/refinery/quickstart#weak-supervision", "http://docs.kern.ai/refinery/quickstart#evaluation", "http://docs.kern.ai/refinery/quickstart#creating-a-validation-data-slice", "http://docs.kern.ai/refinery/quickstart#exporting-the-data", "http://docs.kern.ai/refinery/quickstart#what-to-do-next?", "http://docs.kern.ai/refinery/record-ide#record-ide", "http://docs.kern.ai/refinery/record-ide#writing-code", "http://docs.kern.ai/refinery/record-ide#quality-of-life", "http://docs.kern.ai/refinery/record-ide#search-in-bricks", "http://docs.kern.ai/refinery/record-ide#saving-and-loading-code", "http://docs.kern.ai/refinery/updating-refinery#updating-refinery", "http://docs.kern.ai/refinery/weak-supervision#weak-supervision", "http://docs.kern.ai/refinery/weak-supervision#what-is-weak-supervision?", "http://docs.kern.ai/refinery/weak-supervision#use-in-refinery", "http://docs.kern.ai/bricks/classifiers#classifiers", "http://docs.kern.ai/bricks/extractors#extractors", "http://docs.kern.ai/bricks/generators#generators", "http://docs.kern.ai/bricks#bricks", "http://docs.kern.ai/bricks#execution-types", "http://docs.kern.ai/bricks#features-of-bricks", "http://docs.kern.ai/bricks#customizable", "http://docs.kern.ai/bricks#integration-to-refinery", "http://docs.kern.ai/bricks#growing-weekly", "http://docs.kern.ai/bricks#open-source", "http://docs.kern.ai/bricks#what's-next?", "http://docs.kern.ai/bricks/refinery-integration#integration-to-refinery", "http://docs.kern.ai/gates/deploy#deploy", "http://docs.kern.ai/gates/deploy#requirements", "http://docs.kern.ai/gates/deploy#setting-up-the-model", "http://docs.kern.ai/gates/deploy#the-gates-api-{{-tag:-'post'-label:-'/commercial/v1/predict/project/:id'-}}", "http://docs.kern.ai/gates/deploy#replace-with-your-own-example-data", "http://docs.kern.ai/gates/deploy#what's-next?", "http://docs.kern.ai/gates#gates", "http://docs.kern.ai/gates#features-of-gates", "http://docs.kern.ai/gates#3-clicks-to-deploy-your-model", "http://docs.kern.ai/gates#secure-execution---anywhere", "http://docs.kern.ai/gates#monitor-requests-per-hour-confidence-and-runtime", "http://docs.kern.ai/gates#containerized-runtime", "http://docs.kern.ai/gates/monitoring#monitoring", "http://docs.kern.ai/gates/token#access-token", "http://docs.kern.ai/workflow/access-token#access-token", "http://docs.kern.ai/workflow/access-token#fetching-workflow-data", "http://docs.kern.ai/workflow/access-token#fetching-and-adding-store-data", "http://docs.kern.ai/workflow/identities#identities", "http://docs.kern.ai/workflow#workflow", "http://docs.kern.ai/workflow#structure-of-this-documentation", "http://docs.kern.ai/workflow#featured-use-cases", "http://docs.kern.ai/workflow#features-of-workflow", "http://docs.kern.ai/workflow#drag-and-drop-editor", "http://docs.kern.ai/workflow#completing-the-stack", "http://docs.kern.ai/workflow#integrations", "http://docs.kern.ai/workflow#data-collection", "http://docs.kern.ai/workflow#realtime-and-batch", "http://docs.kern.ai/workflow/quickstart#quickstart", "http://docs.kern.ai/workflow/quickstart#creating-a-first-workflow", "http://docs.kern.ai/workflow/quickstart#setting-up-a-store", "http://docs.kern.ai/workflow/quickstart#creating-and-connecting-nodes", "http://docs.kern.ai/workflow/quickstart#what's-next?", "http://docs.kern.ai/workflow/stores#stores", "http://docs.kern.ai/workflow/stores#store-options", "http://docs.kern.ai/workflow/stores#adding-stores", "http://docs.kern.ai/workflow/stores#maintaining-stores", "http://docs.kern.ai/workflow/stores#synchronizing-a-store-with-refinery", "http://docs.kern.ai/workflow/stores#store-api", "http://docs.kern.ai/workflow/stores#the-store-model", "http://docs.kern.ai/workflow/stores#properties", "http://docs.kern.ai/workflow/stores#the-store-entry-model", "http://docs.kern.ai/workflow/stores#properties", "http://docs.kern.ai/workflow/stores#retrieve-a-store-{{-tag:-'get'-label:-'/workflow-api/stores/:id'-}}", "http://docs.kern.ai/workflow/stores#post-an-entry-{{-tag:-'post'-label:-'/workflow-api/stores/:id/add-entry'-}}", "http://docs.kern.ai/workflow/variables#variables", "http://docs.kern.ai/workflow/workflows#workflows", "http://docs.kern.ai/workflow/workflows#workflow-options", "http://docs.kern.ai/workflow/workflows#the-canvas", "http://docs.kern.ai/workflow/workflows#the-node-palette", "http://docs.kern.ai/workflow/workflows#source-nodes", "http://docs.kern.ai/workflow/workflows#modifier-nodes", "http://docs.kern.ai/workflow/workflows#target-nodes", "http://docs.kern.ai/workflow/workflows#gates-ai-node", "http://docs.kern.ai/workflow/workflows#connecting-nodes", "http://docs.kern.ai/workflow/workflows#running-a-workflow", "http://docs.kern.ai/workflow/workflows#logging-nodes", "http://docs.kern.ai/workflow/workflows#using-environment-variables", "http://docs.kern.ai/workflow/workflows#complex-workflows", "http://docs.kern.ai/workflow/workflows#monitoring-workflows", "http://docs.kern.ai/workflow/workflows#workflow-api", "http://docs.kern.ai/workflow/workflows#the-workflow-model", "http://docs.kern.ai/workflow/workflows#properties", "http://docs.kern.ai/workflow/workflows#the-node-model", "http://docs.kern.ai/workflow/workflows#properties", "http://docs.kern.ai/workflow/workflows#retrieve-a-workflow-{{-tag:-'get'-label:-'/workflow-api/workflows/:id'-}}", "http://docs.kern.ai/workflow/workflows#get-the-throughput-{{-tag:-'post'-label:-'/workflow-api/workflows/:id/throughput'-}}"]}